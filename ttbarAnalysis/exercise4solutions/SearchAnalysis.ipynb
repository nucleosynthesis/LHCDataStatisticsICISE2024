{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bc23b-22f6-4292-af16-333526e75dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, BaseSchema\n",
    "from agc_schema import AGCSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4f9bd-7317-421d-b944-271adacb322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('ntuples_with_tprime.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Function to calculate the integrated luminosity in case we couldn't access all of the files\n",
    "def getLumi(nevts):\n",
    "    lumi_nom = 2256.38 # total possible luminosity in /pb\n",
    "    sf = float(nevts)/36619019\n",
    "    return lumi_nom*sf \n",
    "\n",
    "# Useful function to calculate weight, pass the sample name, \n",
    "# total_events summed over files accessed and lumi calculated from function above\n",
    "def getEventWeight(sample,n_total_events,lumi):\n",
    "    if sample != \"data\": #Â we don't weight the real data\n",
    "        xsec_weight = metadata[sample]['filter_eff']*metadata[sample]['xsec'] * lumi / (n_total_events)\n",
    "    else:\n",
    "        xsec_weight = 1\n",
    "    return xsec_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481923c-e666-4b78-982c-cafb7b486260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the binning for my observable (m_bjj)\n",
    "obsbins=np.arange(500,2080,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d67d9a-0032-4925-a355-acaac047c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will run over the analysis for any input file \n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def runAnalysis(input_file,isMC=True):\n",
    "    try: \n",
    "        events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\n",
    "    except OSError:\n",
    "        time.sleep(2) # sleep for 2 seconds and try one more time\n",
    "        try: \n",
    "            events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\n",
    "        except OSError:\n",
    "            time.sleep(2) # sleep for 2 seconds just to not try EOS again but give up on this file now\n",
    "            return [],[]\n",
    "    \n",
    "    # Object selection\n",
    "    selected_electrons = events.electron[(events.electron.pt > 30) & (abs(events.electron.eta)<2.1) & (events.electron.isTight == True) & (events.electron.sip3d < 4 )]\n",
    "    selected_muons = events.muon[(events.muon.pt > 30) & (abs(events.muon.eta)<2.1) & (events.muon.isTight == True) & (events.muon.sip3d < 4) & (events.muon.pfreliso04DBCorr < 0.15)]\n",
    "    selected_jets = events.jet[(events.jet.corrpt > 30) & (abs(events.jet.eta)<2.4) ]\n",
    "    \n",
    "    # Event selection\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    \n",
    "    # Separate event selection into signal region and control region \n",
    "    event_filters_CR = event_filters &  (ak.count(selected_jets.corrpt, axis=1) >= 4) & (ak.sum(selected_jets.btag >= 0.8, axis=1) < 1)\n",
    "    event_filters_SR = event_filters &  (ak.count(selected_jets.corrpt, axis=1) >= 4) & (ak.sum(selected_jets.btag >= 0.8, axis=1) >= 2)\n",
    "    \n",
    "    selected_events_SR = selected_jets[event_filters_SR]\n",
    "    selected_events_CR = selected_jets[event_filters_CR]\n",
    "    \n",
    "    # calculate scale up/down \n",
    "    if isMC:\n",
    "        selected_events_SR['scaleUp']   = selected_events_SR.corrptUp/selected_events_SR.corrpt\n",
    "        selected_events_SR['scaleDown'] = selected_events_SR.corrptDown/selected_events_SR.corrpt\n",
    "        \n",
    "        selected_events_CR['scaleUp']   = selected_events_CR.corrptUp/selected_events_CR.corrpt\n",
    "        selected_events_CR['scaleDown'] = selected_events_CR.corrptDown/selected_events_CR.corrpt\n",
    "    \n",
    "    # Calculate observable for SR (mbjj)\n",
    "    trijet = ak.combinations(selected_events_SR, 3, fields=[\"j1\", \"j2\", \"j3\"])\n",
    "    j1,j2,j3 = ak.unzip(trijet)\n",
    "    has_bjet = (ak.sum(j1.btag>=0.8,axis=1)+ak.sum(j2.btag>=0.8,axis=1)+ak.sum(j3.btag>=0.8,axis=1)>0)\n",
    "    trijet = trijet[has_bjet]\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observableSR = ak.flatten(trijet_mass).to_numpy()\n",
    "     \n",
    "    if isMC:\n",
    "        trijet[\"p4Up\"]   = trijet.j1.multiply(trijet.j1.scaleUp) +trijet.j2.multiply(trijet.j2.scaleUp)+trijet.j3.multiply(trijet.j3.scaleUp) \n",
    "        trijet[\"p4Down\"] = trijet.j1.multiply(trijet.j1.scaleDown) +trijet.j2.multiply(trijet.j2.scaleDown)+trijet.j3.multiply(trijet.j3.scaleDown) \n",
    "        \n",
    "        trijet_massU = trijet[\"p4Up\"][ak.argmax(trijet.p4Up.pt, axis=1, keepdims=True)].mass\n",
    "        trijet_massD = trijet[\"p4Down\"][ak.argmax(trijet.p4Down.pt, axis=1, keepdims=True)].mass\n",
    "    \n",
    "        observableSRU = ak.flatten(trijet_massU).to_numpy()\n",
    "        observableSRD = ak.flatten(trijet_massD).to_numpy()\n",
    "    \n",
    "    \n",
    "    # Calculate observable for CR (mjjj)\n",
    "    trijet = ak.combinations(selected_events_CR, 3, fields=[\"j1\", \"j2\", \"j3\"])\n",
    "    j1,j2,j3 = ak.unzip(trijet)\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observableCR = ak.flatten(trijet_mass).to_numpy()\n",
    "    \n",
    "    if isMC:\n",
    "        trijet[\"p4Up\"]   = trijet.j1.multiply(trijet.j1.scaleUp) +trijet.j2.multiply(trijet.j2.scaleUp)+trijet.j3.multiply(trijet.j3.scaleUp) \n",
    "        trijet[\"p4Down\"] = trijet.j1.multiply(trijet.j1.scaleDown) +trijet.j2.multiply(trijet.j2.scaleDown)+trijet.j3.multiply(trijet.j3.scaleDown) \n",
    "        \n",
    "        trijet_massU = trijet[\"p4Up\"][ak.argmax(trijet.p4Up.pt, axis=1, keepdims=True)].mass\n",
    "        trijet_massD = trijet[\"p4Down\"][ak.argmax(trijet.p4Down.pt, axis=1, keepdims=True)].mass\n",
    "    \n",
    "        observableCRU = ak.flatten(trijet_massU).to_numpy()\n",
    "        observableCRD = ak.flatten(trijet_massD).to_numpy()\n",
    "    \n",
    "    # create counts in bins \n",
    "    counts_bin_edgesSR  = plt.hist(observableSR,bins=obsbins)\n",
    "    countsSR, bin_edgesSR = counts_bin_edgesSR[0],counts_bin_edgesSR[1]\n",
    "    \n",
    "    if isMC : \n",
    "        countsSRU_bin_edgesSR  = plt.hist(observableSRU,bins=obsbins)\n",
    "        countsSRU, bin_edgesSR = countsSRU_bin_edgesSR[0],countsSRU_bin_edgesSR[1]\n",
    "    \n",
    "        countsSRD_bin_edgesSR  = plt.hist(observableSRD,bins=obsbins)\n",
    "        countsSRD, bin_edgesSR = countsSRD_bin_edgesSR[0],countsSRD_bin_edgesSR[1]\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    counts_bin_edgesCR  = plt.hist(observableCR,bins=obsbins)\n",
    "    countsCR, bin_edgesCR = counts_bin_edgesCR[0],counts_bin_edgesCR[1]\n",
    "\n",
    "   \n",
    "    if isMC: \n",
    "        countsCRU_bin_edgesCR  = plt.hist(observableCRU,bins=obsbins)\n",
    "        countsCRU, bin_edgesCR = countsCRU_bin_edgesCR[0],countsCRU_bin_edgesCR[1]\n",
    "    \n",
    "        countsCRD_bin_edgesCR  = plt.hist(observableCRD,bins=obsbins)\n",
    "        countsCRD, bin_edgesCR = countsCRD_bin_edgesCR[0],countsCRD_bin_edgesCR[1]\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    # return the counts \n",
    "    # return the counts \n",
    "    if isMC: \n",
    "        return {'nominal':np.array(countsSR),'jesUp':np.array(countsSRU),'jesDown':np.array(countsSRD)},{'nominal':np.array(countsCR),'jesUp':np.array(countsCRU),'jesDown':np.array(countsCRD)}\n",
    "    else : return {'nominal':np.array(countsSR)},{'nominal':np.array(countsCR)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d053eec-9b8d-46ae-9f7c-40a23c30d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hist2df import * \n",
    "\n",
    "samples=[\n",
    "    'data'\n",
    "    ,'ttbar'\n",
    "    ,'single_atop_t_chan'\n",
    "    ,'single_top_t_chan'\n",
    "    ,'single_top_tW'\n",
    "    ,'wjets'\n",
    "    ,'tprime'\n",
    "]\n",
    "\n",
    "\n",
    "n_sim_samples = len(samples)-1\n",
    "\n",
    "# first run over the data. We want to keep track of the total number of events \n",
    "# that get accessed so that we can modify the integrated luminosity if necessary \n",
    "\n",
    "print('data')\n",
    "countsSR = {'nominal':np.array([0. for i in range(len(obsbins)-1)])}\n",
    "countsCR = {'nominal':np.array([0. for i in range(len(obsbins)-1)])}\n",
    "total_events = 0\n",
    "\n",
    "MAX_FILES = -1 # if a negative number, then we don't set a maximum on the number of files. \n",
    "\n",
    "for j,p in enumerate(metadata['data']['nominal']['files']): \n",
    "    \n",
    "    # for each file, we add the counts from these events\n",
    "    if (MAX_FILES > 0 and j > MAX_FILES): continue\n",
    "    print(j,p['path'])\n",
    "\n",
    "    event_countsSR,event_countsCR = runAnalysis(p['path'],isMC=False)\n",
    "    if len(event_countsSR):\n",
    "        countsSR['nominal'] += event_countsSR['nominal']\n",
    "        countsCR['nominal'] += event_countsCR['nominal']\n",
    "        total_events+=p['nevts'] \n",
    " \n",
    "    else : print(\"skipped file (XRootD error)\")\n",
    "    \n",
    "\n",
    "intlumi = getLumi(total_events)\n",
    "print(\"Data total events accessed\", total_events, \" -> %g /pb\"%intlumi )\n",
    "                 \n",
    "# convert to dataframe \n",
    "dfs   = histogramToDataframe(countsSR['nominal'],\"signalregion\",'data')\n",
    "dfsCR = histogramToDataframe(countsCR['nominal'],\"controlregion\",'data')\n",
    "dfs = pd.concat([dfs,dfsCR],ignore_index=True)\n",
    "                \n",
    "# Now we run over the simulated samples. Again, we should keep track of the number of events  \n",
    "# that get accessed so that we can calculate the right event_weight\n",
    "\n",
    "for i,sample in enumerate(samples[1:]): \n",
    "    if sample=='data': continue # shouldn't need this \n",
    "    print(sample)\n",
    "    \n",
    "     # make sure to zero out the counts for each sample \n",
    "    countsSR = {\n",
    "         'nominal':np.array([0. for i in range(len(obsbins)-1)])\n",
    "        ,'jesUp':np.array([0. for i in range(len(obsbins)-1)])\n",
    "        ,'jesDown':np.array([0. for i in range(len(obsbins)-1)])\n",
    "    }\n",
    "    countsCR = {\n",
    "         'nominal':np.array([0. for i in range(len(obsbins)-1)])\n",
    "        ,'jesUp':np.array([0. for i in range(len(obsbins)-1)])\n",
    "        ,'jesDown':np.array([0. for i in range(len(obsbins)-1)])\n",
    "    }\n",
    "    total_sim_events = 0\n",
    "    \n",
    "    for j,p in enumerate(metadata[sample]['nominal']['files']): \n",
    "        \n",
    "        if (MAX_FILES > 0 and j > MAX_FILES): continue\n",
    "        print(j,p['path'])\n",
    "        # for each file, we add the counts from these events\n",
    "        event_countsSR,event_countsCR = runAnalysis(p['path'],isMC=True)\n",
    "        if len(event_countsSR):\n",
    "            countsSR['nominal'] += event_countsSR['nominal']\n",
    "            countsCR['nominal'] += event_countsCR['nominal']\n",
    "            \n",
    "            countsSR['jesUp'] += event_countsSR['jesUp']\n",
    "            countsCR['jesUp'] += event_countsCR['jesUp']\n",
    "            \n",
    "            countsSR['jesDown'] += event_countsSR['jesDown']\n",
    "            countsCR['jesDown'] += event_countsCR['jesDown']\n",
    "            total_sim_events+=p['nevts'] \n",
    "           \n",
    "        else : print(\"skipped file (XRootD error)\")\n",
    "        \n",
    "            \n",
    "    event_weight = getEventWeight(sample,total_sim_events,intlumi)\n",
    "    print(\"Total events processed (%s):\"%(sample), total_sim_events, \n",
    "          \"XSec = \",metadata[sample]['xsec'], \n",
    "          \"Filter efficiency = \", metadata[sample]['filter_eff'], \n",
    "          \"Event weight = \", event_weight)\n",
    "        \n",
    "    # now that we have the counts, we convert to a dataframe \n",
    "    dfs_sampleSRN = histogramToDataframe(countsSR['nominal'],\"signalregion\",sample)\n",
    "    dfs_sampleCRN = histogramToDataframe(countsCR['nominal'],\"controlregion\",sample)\n",
    "    \n",
    "    dfs_sampleSRU = histogramToDataframe(countsSR['jesUp'],\"signalregion\",sample,sys='jesUp')\n",
    "    dfs_sampleCRU = histogramToDataframe(countsCR['jesUp'],\"controlregion\",sample,sys='jesUp')\n",
    "    \n",
    "    dfs_sampleSRD = histogramToDataframe(countsSR['jesDown'],\"signalregion\",sample,sys='jesDown')\n",
    "    dfs_sampleCRD = histogramToDataframe(countsCR['jesDown'],\"controlregion\",sample,sys='jesDown')\n",
    "    \n",
    "    dfs_sample = pd.concat([dfs_sampleSRN,dfs_sampleCRN,dfs_sampleSRU,dfs_sampleCRU,dfs_sampleSRD,dfs_sampleCRD])\n",
    "    \n",
    "    # apply weight to columns\n",
    "    dfs_sample['sum_w']*=event_weight\n",
    "    dfs_sample['sum_ww']*=event_weight*event_weight\n",
    "    \n",
    "    # add the new samples into our dataframe for signal and control regions\n",
    "    dfs = pd.concat([dfs,dfs_sample],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2aa3b3-1e85-4e49-86e1-1f0c1c5381ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "order_sim = samples[1:-1] # don't include the signal in the stack\n",
    "order_sim.reverse() # plot stack in reverse \n",
    "n_sim_samples = len(order_sim)\n",
    "\n",
    "# need to make list structures for this to work in stacked mode \n",
    "bin_centres = np.array(obsbins)+(obsbins[1]-obsbins[0])/2\n",
    "bins_list = [obsbins for b in range(n_sim_samples)]\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,6))\n",
    "\n",
    "def mkPlot(axis,region,obsname,title):\n",
    "    \n",
    "    samples_stack = [dfs[ (dfs['process']==s) & (dfs['systematic']=='nominal') & (dfs['channel']==region)]['sum_w'].to_list() for s in order_sim]\n",
    "    labels = order_sim\n",
    "\n",
    "\n",
    "    axis.hist(bins_list,bins=obsbins,weights=samples_stack,label=labels,stacked=True,density=False)\n",
    "    axis.errorbar(bin_centres,dfs[ (dfs['process']=='data') & (dfs['systematic']=='nominal') & (dfs['channel']==region)]['sum_w']\n",
    "             ,yerr     =(dfs[ (dfs['process']=='data') & (dfs['systematic']=='nominal') & (dfs['channel']==region)]['sum_ww'])**0.5\n",
    "             ,label='data (%.2f $fb^{-1}$)'%(intlumi/1000)\n",
    "             ,marker='o'\n",
    "             ,markersize=3.2\n",
    "             ,color='black'\n",
    "             ,linestyle=\"none\")\n",
    "    \n",
    "    axis.hist(obsbins,bins=obsbins\n",
    "         ,weights=dfs[ (dfs['process']=='tprime') & (dfs['systematic']=='nominal') & (dfs['channel']==region)]['sum_w']\n",
    "         ,color='cyan'\n",
    "         ,histtype='step'\n",
    "         ,linewidth=3\n",
    "         ,label='tprime - $m_{T}=1.2$ TeV'\n",
    "        )\n",
    "    axis.set_xlabel(obsname) #\"$m_{bjj}$ (GeV)\")\n",
    "    axis.set_ylabel(\"Events\")\n",
    "    #axis.set_yscale(\"log\")\n",
    "    axis.set_title(title)\n",
    "    axis.legend()\n",
    "\n",
    "mkPlot(ax1,\"signalregion\",\"$m_{bjj}$ (GeV)\",\"4j2b\")\n",
    "mkPlot(ax2,\"controlregion\",\"$m_{jjj}$ (GeV)\",\"4j0b\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"TPrimesearch_dists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e2375-4971-435a-850f-5edbd88635b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe to avoid warnings later\n",
    "dfs = dfs.sort_values(by=['channel','process','systematic','bin'])\n",
    "\n",
    "# Finally save histograms to a csv file\n",
    "dfs.to_csv('tprime_1200GeV_search_allregions_mbjj.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb15f3-dc84-4ee4-9736-af577ac9648d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
