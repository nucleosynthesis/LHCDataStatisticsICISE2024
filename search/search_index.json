{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"INTRODUCTION","text":"<p>This is the webpage for the lecture series on Data Analysis and Statistics at the LHC. In these pages, you can find links to the lectures as well as exercises for you to explore a typical data analysis at CMS.  You should be familiar with simple programming in Python, however, the examples should give you enough information to produce the desired output even if you don't know much Python. </p> <p>You should start by downloading the Docker desktop for your laptop (click here). You will need to setup an account to do so. Follow the instructions for setting up the software that we'll be using at the Getting started tab. This can take some time so please do this before the start of the lecture course. </p> <p>There are four sets of exercises in these pages that you can find under the Exercises tab, one for each day of the course. You should read through the information given and copy code/commands where prompted into your own terminal or notebook. </p> <p>Question</p> <p>Throughout the exercises, there will be questions in a green box like this that contains a challenge for you to try on your own. Below, there will be a solution in a blue box for you to check against your own answer or in case you get really stuck. Please do try on your own before revealing the solution. </p> Show answer The answer will be shown here  <p>Challenge</p> <p>If you are feeling especially confident, you can have a go at extra challenges that I have put in these turquoise boxes. I won't provide a solution for these challenges, but you don't need to complete them to progress through the other exercises. </p> <p>Warning</p> <p>Sometimes, there will be some technical or conceptual hurdle that requires a bit of careful thought to work around. I have tried to point these out to you in orange boxes like this one to help avoid any major road blocks. </p>"},{"location":"#useful-links","title":"Useful Links","text":"<p>You can find more useful information about the software we will be using below </p> <ul> <li>Docker: Docker is a desktop client for running the container images that we will be using for these exercises.  </li> <li>CMS Open Data: The CMS Collaboration regularly releases open datasets for public use. This link is the CMS Open Data Guide</li> <li>Combine: The Combine tool is the main statistical analysis software package used by the CMS Collaboration. It is based on the ROOT/RooFit software packages, however for these exercises, you do not need to be an expert user of these packages. </li> </ul>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Dr. Nicholas Wardle is a lecturer in Physics at Imperial College London. He is also a member of the CMS Collaboration and his research is focused on statistical methods for data analysis and searches for BSM physics through precision measurements of Higgs boson properties. </p>"},{"location":"day1/","title":"Exercise 1 - Datasets, Selections and Histograms","text":"<p>For the exercises, we will use a simplified version of a CMS analysis to measure the production cross-section of top-quark / anti-top-quark pairs - \\sigma_{tt} - in proton-proton collisions at a centre of mass energy of 13 TeV. We will be using real data from the CMS experiment, taken in 2015 during Run 2 of the LHC. The code is based on the CMS open data workshop linked here, which you can read through if you are interested. </p> <p>The image below is a Feynman diagram of the process we are interested in measuring, namely t\\bar{t}\\rightarrow (bW^{+})(\\bar{b}W^{-})\\rightarrow bl^{+}\\nu_{l}\\,\\bar{b}q\\bar{q} - the lepton+jets final state. </p> <p></p> <p>Of course, the charge conjugate version (with the sign of the charged lepton flipped) is also possible. In the following, we will consider both versions so don't get too hung up on the charges of the particles in the final state. We'll drop the charge and anti-particle symbols from here-on. </p> <p>In our example, we will focus on the case where the lepton l is an electron or muon.  In the CMS detector, these leptons are extremely well measured and are very good for \"triggering\" the events that we are interested in. Looking at the final state, we would also expect to see 4 hadronic jets, two of which will be b-jets, and missing transverse momentum from the neutrino which isn't detected.  </p>"},{"location":"day1/#launch-jupyterlab","title":"Launch JupyterLab","text":"<p>For this exercise, we will be using data from the LHC Run-2 proton-proton collision data recorded by CMS during 2015.  In addition to the data, we will be using simulated samples of tt\\rightarrow (bW)(bW)\\rightarrow bl\\nu_{l}\\,bqq production and various background processes that can mimic the signal.  </p> <p>First, start the <code>cms_python</code> container using  <pre><code>docker start -i cms_python\n</code></pre> Note that you should have the Docker desktop running on your laptop for this to work. If you didn't install the container already, please see the Getting started pages before continuing. </p> <p>Next, you should checkout the GitHub area that contains some helpful files and information for accessing and processing the data. In the same terminal as you started the container in type,  <pre><code>git clone https://github.com/nucleosynthesis/LHCDataStatisticsICISE2024.git\ncd LHCDataStatisticsICISE2024/ttbarAnalysis \n</code></pre></p> <p>Now you can start the JupyterLab server by running  <pre><code>jupyter lab --ip 0.0.0.0 --port 8888 --no-browser\n</code></pre></p> <p>The output will give you a link (starting with <code>http://</code>) that you can paste into your preferred internet browser to open the JuypterLab session. Create a new Python 3 notebook - icon that says Python 3 (ipykernel) - and give it a name by right clicking in the file browser at the left panel where it says \"Untitled.ipynb\". We will use this notebook to issue commands to process the data. </p> <p>You can create new cells in the notebook by using the \"+\" icon at the top. The code that is entered in each cell can be run by either clicking the play icon at the top or by selecting the cell and pressing Shift + Enter on your keyboard. Jupyter notebooks are a great way of editing code and displaying output all in one place! </p>"},{"location":"day1/#accessing-and-exploring-the-data-sets","title":"Accessing and exploring the data sets","text":"<p>In your JupyterLab file browser, you will see the list of files that were obtained from GitHub. There is a file called <code>ntuples.json</code> that contains the file paths for the different samples (data and simulation) that we'll use in these exercises. </p> <p>In the file browser, if you click on the <code>ntuple.json</code> file, it will open in the main panel. You can click on the different names to expand them and see the information contained in the file. It should look like the image below, </p> <p></p> <p>These datasets have been skimmed by applying certain selection criteria. This is to make the samples more manageable as the original datasets are TBs in size! The requirements applied are, </p> <ul> <li>That the events fired at least one of these triggers: <code>HLT_Ele22_eta2p1_WPLoose_Gsf</code>, <code>HLT_IsoMu20_v</code>, <code>HLT_IsoTkMu20_v</code>. These essentially require that the online selection (trigger) found at least one isolated electron or one isolated muon with a minimum p_{T}, in a certain part of the CMS detector (can you figure out from the names, what the p_{T} requirements at the online selection are?). </li> <li>That the event contains either at least one tight electron or at least one tight muon with p_{T}&gt; 26 GeV and |\\eta|&lt;2.1. When we use the word tight in this context, it is jargon internal to CMS that just specifies how pure we are trying to make the sample. A tight selection criteria would be quite pure and have fewer false positives than a medium or loose selection which might retain more true muons or electrons, but also have more false positives (fake electrons or muons). </li> </ul> <p>The data are saved as <code>.root</code> files. Don't worry if you are not familiar with the ROOT software, we will some very nice Python tools from the Hep Software Foundation to read these files and convert them into Python based formats. </p> <p>Copy the code below into a cell in your Jupyter notebook. This imports the various modules that we need to read and convert our data into Python formats. </p> <pre><code>import uproot\nimport numpy as np\nimport awkward as ak\nfrom coffea.nanoevents import NanoEventsFactory, BaseSchema\nfrom agc_schema import AGCSchema\n</code></pre> <p>Run the cell to execute the commands. You should see the cell number (eg <code>[1]</code>) become an asterisk (<code>[*]</code>) for a short while, which indicates that the cell is running. Once the commands have finished, the number will appear again. </p> <p>Next, let's open one of the files from the <code>ttbar</code> sample. Copy the command below into the next cell of your Jupyter notebook and run it. </p> <pre><code>events = NanoEventsFactory.from_root('root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/RunIIFall15MiniAODv2_TT_TuneCUETP8M1_13TeV-powheg-pythia8_flat/00EFE6B3-01FE-4FBF-ADCB-2921D5903E44_flat.root', schemaclass=AGCSchema, treepath='events').events()\n\nprint(events.fields)\n</code></pre> <p>Warning</p> <p>You may find that sometimes you will see an <code>IOError</code> due to XRootD failing to access the remote files. This can be annoying so I recommend that you split up the task across different cells and that you wrap the command to open the file with a <code>try</code> <code>except</code> block similar to,  <pre><code>input_file = \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/Run2015D_SingleMuon_flat/07FC2CD2-106C-4419-9260-12B3E271C345_flat.root\"\ntry: \n    events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\nexcept OSError:\n    time.sleep(2) # sleep for 2 seconds and try one more time\n    try: \n        events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\n    except OSError:\n        time.sleep(2) # sleep for 2 seconds just to not try EOS again but give up on this file now\n        return []\n</code></pre> This will try to access a file twice before giving up and returning an empty list.</p> <p>The output should be a list of the different collections (fields) that are contained in the data. Each event may have a different number of objects in each collection. We can see this by entering the following code in the Jupyter notebook cell, </p> <pre><code>print(\"Electrons:\")\nprint(f\"Number of events: {ak.num(events.electron, axis=0)}\")\nprint(ak.num(events.electron, axis=1))\nprint()\n</code></pre> <p>The output should look like  <pre><code>Electrons:\nNumber of events: 325519\n[3, 2, 2, 1, 1, 2, 0, 1, 2, 3, 1, 1, 3, 4, ... 4, 1, 1, 2, 5, 3, 1, 2, 2, 2, 1, 2, 1]\n</code></pre></p> <p>From this we can see that there are 325519 events in total in this file and each event has a different number of electrons. Notice that we used the same <code>ak.num</code> command in each printout but we changed the depth of the sum by specifying <code>axis=</code>. The larger this number, the deeper into the event structure the function goes. </p> <p>Question</p> <p>Do the same for <code>muon</code>, <code>jet</code> and <code>met</code> in each event, to see how many of these objects are contained in each event. Why do you get an error when using the function for <code>met</code>, the missing transverse momentum of the event?  </p> Show answer  Similarly to the case for electrons, we can use the following,  <pre><code>print(\"Muons:\")\nprint(f\"Number of events: {ak.num(events.muon, axis=0)}\")\nprint(ak.num(events.muon, axis=1))\nprint()\n\nprint(\"Jets:\")\nprint(f\"Number of events: {ak.num(events.jet, axis=0)}\")\nprint(ak.num(events.jet, axis=1))\n</code></pre>  Which yields the output,  <pre><code>Muons:\nNumber of events: 325519\n[1, 1, 1, 0, 6, 6, 3, 2, 1, 2, 2, 6, 4, 2, ... 2, 1, 1, 2, 0, 5, 2, 3, 2, 2, 2, 2, 1]\n\nJets:\nNumber of events: 325519\n[4, 5, 4, 5, 9, 4, 4, 8, 5, 9, 5, 4, 6, 6, ... 6, 2, 4, 3, 5, 9, 4, 7, 4, 4, 6, 5, 4]\n</code></pre>  When doing the same for <code>met</code>, we get an error  <pre><code>ValueError: 'axis' out of range for 'num'\n</code></pre>  The reason is that the <code>met</code> field is an event level variable meaning it is calculated only once per event."},{"location":"day1/#applying-a-basic-analysis-selection","title":"Applying a basic analysis selection","text":"<p>We will apply selections to the events to discard those that are unlikely to have arisen from our target (signal) process t\\bar{t}\\rightarrow (bW^{+})(bW^{-})\\rightarrow bq\\bar{q}bl^{-}\\bar{\\nu}_{l}. </p> <p>Looking at the process, its clear that we should select events that have one muon or one electron, along with at least four jets which are either from a light flavour quark or from a b-quark (q or b). </p> <p>Generally, when applying selections to events, we first think about object selection - i.e we only want to apply event selections using well identified objects, and then event selection where we really target the final state that we are looking for. </p>"},{"location":"day1/#object-selection","title":"Object selection","text":"<p>We can see which features each of our collections have by checking the <code>fields</code> available. Copy the following code into a Juptyer notebook cell,  <pre><code>print(\"Electron fields:\")\nprint(events.electron.fields)\nprint()\n</code></pre> This should give the following output, </p> <pre><code>Electron fields:\n['pt', 'px', 'py', 'pz', 'eta', 'phi', 'ch', 'iso', 'veto', 'isLoose', 'isMedium', 'isTight', 'dxy', 'dz', 'dxyError', 'dzError', 'ismvaLoose', 'ismvaTight', 'ip3d', 'sip3d', 'energy']\n</code></pre> <p>We can see that the electrons have fields corresponding to, </p> <ul> <li>The kinematics of the electron: transverse momentum <code>pt</code> (p_{T}), pseudo-rapidity <code>eta</code> (\\eta)</li> <li>Identification quality information : Whether the electron passes loose, medium or tight selection requirements : <code>isLoose</code>, <code>isMedium</code>, <code>isTight</code>. </li> <li>Other features related to the track-calorimeter matching and how well isolated the electron object is. </li> </ul> <p>We can apply requirements on the objects by applying masks to our event collections. For electrons, we will apply the following criteria to the electron fields, </p> <ul> <li>p_{T}&gt; 30 GeV, |\\eta|&lt;2.1</li> <li>passing the tight identification requirements </li> <li>3D impact parameter (<code>sip3d</code>) &lt; 4</li> </ul> <p>Copy the code below to determine which events would pass this selection,  <pre><code>selected_electrons = events.electron[(events.electron.pt &gt; 30) &amp; (abs(events.electron.eta)&lt;2.1) &amp; (events.electron.isTight == True) &amp; (events.electron.sip3d &lt; 4 )]\n</code></pre></p> <p>Question</p> <p>Create the same object masks for selected muons and selected jets based on the following criteria, </p> <p>For muons: </p> <ul> <li>p_{T}&gt; 30 GeV, |\\eta|&lt;2.1 </li> <li>passing the tight identification requirements  </li> <li>3D impact parameter (<code>sip3d</code>) &lt; 4</li> <li>Relative isolation requirement (<code>pfreliso04DBCorr</code>) &lt; 0.15.</li> </ul> <p>For jets: </p> <ul> <li>Corrected (<code>corrpt</code>) p_{T}&gt; 30 GeV and |\\eta|&lt;2.4  The corrected p_{T} is the one obtained after applying a number of calibrations to the jets. </li> </ul> Show answer <pre><code>selected_muons = events.muon[(events.muon.pt &gt; 30) &amp; (abs(events.muon.eta)&lt;2.1) &amp; (events.muon.isTight == True) &amp; (events.muon.sip3d &lt; 4) &amp; (events.muon.pfreliso04DBCorr &lt; 0.15)]\nselected_jets = events.jet[(events.jet.corrpt &gt; 30) &amp; (abs(events.jet.eta)&lt;2.4) ]\n</code></pre>"},{"location":"day1/#event-selection","title":"Event selection","text":"<p>Now we want to apply the event level filters to our events. To do this, we use the <code>ak.count</code> method to count the number of objects of each type that we require in our events. Looking at our final state, our event selection will be, </p> <ul> <li>Exactly one electron or muon that passes the electron/muon requirements </li> <li>At least 4 jets, at least two of which should be a b-jet. </li> </ul> <p>For the first requirement, copy the following code into a Jupyter notebook cell and run it,  <pre><code>event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n</code></pre> The <code>+</code> acts as a logical or so that we are filtering on events that have exactly one electron or exactly one muon. Note that we could use another field of the object but here we have chosen to use <code>pt</code>. Remember, we want to apply the filter on each electron or muon in the events so the sum needs to be applied to the first layer (<code>axis=1</code>). </p> <p>For the second requirement, we need to define how we tag a b-jet. We use the combined secondary vertex algorithm at CMS to do this. When this quantity is high, this implies that the jet originates from a vertex which is far from the pp interaction point, which indicates that the jet originates from a heavy quark with a long(ish) lifetime - i.e a b-quark. For our selection  we will define b-jets as those for which the  b-tagging variable <code>btag</code> is greater than or equal to 0.8. </p> <p>The following code, which you should copy into a cell in your Jupyter notebook, will include the second requirement into our event filters, </p> <pre><code>event_filters = event_filters &amp;  (ak.count(selected_jets.corrpt, axis=1) &gt;= 4) &amp; (ak.sum(selected_jets.btag &gt;= 0.8, axis=1) &gt;= 2)\n</code></pre> <p>Note that we haven't actually applied any of this selection to our events yet. If we check the length of our event filters, we can see that its still the same size as our events,  <pre><code>print(len(event_filters))\n</code></pre></p> <p>The reason is that this object now contains a boolean flag, one for each event, that indicates whether or not the event satisfies all of the criteria that we used to define the filter. To actually select the events that pass this filter, we just apply the filter to one of our selected object collections. For example, to obtain the collection of jets from all of the events that pass our selection, we can do, </p> <pre><code>selected_events = selected_jets[events_filter]\n</code></pre> <p>As a final step, we want to plot some observable in our events that can separate the signal from the various backgrounds. Choosing observables often requires a lot of thought as physicists as there are several things we want to consider </p> <ul> <li>The observable should have some separation between signal and background to make it easier to extract the process that we are interested in. </li> <li>The observable may be more or less susceptible to experimental effects such as detector resolution, pile-up or be theoretically more prone to divergences in the calculation (this can especially true in jet related observables). All of these tend to imply systematic uncertainties that can reduce the sensitivity. </li> <li>The observable may lend itself to allowing for data-driven estimates of distributions for different backgrounds. This won't be the case here, but this can often be a good motivation for choosing certain variables. </li> </ul> <p>For this analysis, our chosen observable will be the mass of the hadronically decaying top-quark m_{bjj}.  We can use the <code>ak.combinations</code> function to find all combinations of 3-jets (trijet) in the events. For each combination, we will calculate the four-momentum of the 3-jet system and require that at least one of the 3 jets in each trijet is b-tagged. </p> <p>Copy the code below into a Jupyter notebook cell and run it, </p> <pre><code>trijet = ak.combinations(selected_events, 3, fields=[\"j1\", \"j2\", \"j3\"])\n\n# Get the individual jets from each combination \nj1,j2,j3 = ak.unzip(trijet)\n# booleans for which combination has one of the 3 jets b-tagged\nhas_bjet = (ak.sum(j1.btag&gt;=0.8,axis=1)+ak.sum(j2.btag&gt;=0.8,axis=1)+ak.sum(j3.btag&gt;=0.8,axis=1)&gt;0)\n# apply the filter \ntrijet = trijet[has_bjet]\n\ntrijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3 \n</code></pre> <p>Note that in the last command, <code>p4</code> is a special field that has the space-time co-ordinates of a four-vector. This makes our calculations of things like the p_{T} or mass of the trijets very easy. Below, we find the invariant mass of the trijet that has the highest p_{T}. You can find out more about what you can do with these objects at this coffea page. </p> <pre><code>trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n</code></pre> <p>Question</p> <p>Create an observable for the p_{T} of the trijet with the largest p_{T}. </p> Show answer <pre><code>trijet_pt = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].pt\n</code></pre>"},{"location":"day1/#jet-energy-scale-uncertainties","title":"Jet energy scale uncertainties","text":"<p>At the LHC experiments like CMS, one of the most important uncertainties for any analysis that looks at jets in the final state is the jet energy scale uncertainty. Remember that jets are created by clustering different particles together to reconstruct the energy of outgoing quark or gluon in the hard event. This means that lots of calibration is needed for the different detector components to properly estimate this energy. These calibrations come with uncertainties. In CMS, there are many different sources of uncertainty that come under the label \"jet energy scale\" but for this exercise, we'll just assume it can be modelled as one uncertainty. </p> <p>Take a look again at the fields of the jets in the event,  <pre><code>print(events.jet.fields)\n</code></pre></p> <p>You will notice there are two fields called <code>corrptUp</code> and <code>corrptDown</code>. These are the transverse momentum of the jets if we assume that the calibrations of the jet energy scale is shifted up or down by 1 standard deviation of the jet energy scale calibration measurement - i.e resulting from our uncertainty in these calibrations. For all of the simulated samples (everything that isn't data), we should calculate the observable three times, one for the nominal value of the p_{T} of the jets and one each for these variations. </p> <p>To do this, we simply need to make two new fields in our <code>selected_events</code> jets that represent the scale-factor applied to the energy-momentum four-vector of the jets. For the \"Up\" variation, do this with,  <pre><code>selected_events['scaleUp'] = selected_events.corrptUp/selected_events.corrpt\n</code></pre> Now, we can scale the jets in our selected trijets by this scale-factor. We can use the <code>multiply</code> method to scale each component of the jet four-vectors by their <code>scaleUp</code> and use these new four-vectors to calculate the observable.  </p> <pre><code>trijet[\"p4Up\"]   = trijet.j1.multiply(trijet.j1.scaleUp) +trijet.j2.multiply(trijet.j2.scaleUp)+trijet.j3.multiply(trijet.j3.scaleUp) \ntrijet_massUp   = trijet[\"p4Up\"][ak.argmax(trijet.p4Up.pt, axis=1, keepdims=True)].mass\nobservableUp    = ak.flatten(trijet_massUp).to_numpy()\n</code></pre> <p>We can see that the values in the <code>observableUp</code> array are shifted compared to the nominal ones in the <code>observable</code> array,</p> <p><pre><code>print(observable)\nprint(observableUp)\n</code></pre> and we'll see <pre><code>[ 500.74713  291.3941   535.3709  ...  642.0001   783.4518  1441.1604 ]\n[ 506.01013534  294.2633194   539.46490363 ...  648.24548484  789.74960418\n 1450.60702913]\n</code></pre></p> <p>Question</p> <p>Calculate the observable again but this time for the <code>scaleDown</code> variation. </p> Show answer <p>We just need to use the <code>corrptDown</code> field instead, i.e  <pre><code>selected_events['scaleDown'] = selected_events.corrptDown/selected_events.corrpt\n...\ntrijet[\"p4Down\"]   = trijet.j1.multiply(trijet.j1.scaleDown) +trijet.j2.multiply(trijet.j2.scaleDown)+trijet.j3.multiply(trijet.j3.scaleDown) \ntrijet_massDown   = trijet[\"p4Down\"][ak.argmax(trijet.p4Down.pt, axis=1, keepdims=True)].mass\nobservableDown    = ak.flatten(trijet_massDown).to_numpy()\n</code></pre></p>"},{"location":"day1/#histograms-and-saving-output","title":"Histograms and saving output","text":"<p>Now that we have our event observable, we can see what the distribution looks like for these events.  First, we need to remove the array structure of the events so that we can use the plotting tools from matplotlib. We have to import this module too of course,  <pre><code>import matplotlib.pyplot as plt\nobservable = ak.flatten(trijet_mass).to_numpy()\nplt.hist(observable,bins=np.arange(50,575,25))\n</code></pre></p> <p>You should see a plot similar to the one below,  </p> <p>We see a peak close to the top mass (~175 GeV) as expected! The resolution of jets in the CMS detector is not very high so we do not see a very sharp peak, but we are clearly seeing the hadronic top quark here. </p> <p>Question</p> <p>Make a plot of the p_{T} of the trijet instead of the mass. You should be careful to choose appropriate binning for this variable. </p> Show answer <pre><code>plt.hist(observablept,bins=np.arange(0,300,25))\n</code></pre> <p>Challenge</p> <p>In the original CMS paper, the observable used was the invariant mass of the b-jet and the lepton (electron or muon) - m_{bl}. For the case where there is more than 1 b-jet in particular the minimum such mass from all pairings of b-jet with the lepton is used. Create this observable for the selected events and plot a histogram of it for our ttbar sample. </p> <p>As this sample is simulation, we need to apply proper event weights to account for the cross-section in the standard model. Remember that the number of events expected is given by the product of the cross-section and the integrated luminosity. For simulated samples, if we know the cross-section \\sigma and the number of events generated N_{gen}, we can calculate the effective luminosity of the simulated sample as, </p>  L_{eff}  =  N_{gen}/\\sigma  <p>Often, there is a filtering step in either the MC generation or in the skimming step - we don't always manage to process all of the samples. It's important to account for this by scaling up the number of generated events by this filter efficiency \\epsilon. So our formula changes to </p>  L_{eff}  =  \\frac{1}{\\epsilon}\\cdot N_{gen}/\\sigma  <p>The event weight for a particular sample will therefore be the ratio of the integrated luminosity of our real data to the effective luminosity of the sample, </p>  w = L_{int}/L_{eff} =L_{int}\\cdot \\epsilon\\cdot \\sigma/N_{gen}   <p>Have a look again inside the <code>ntuples.json</code> file. You will find that the file contains the number of events for each file in each sample - <code>nevts</code> -  (this is the number generated and processed to make that file), the filter efficiency for the sample (<code>filter_eff</code>) and the cross-section in pb - <code>xsec</code> for that sample. You can read in this data using the code below, </p> <p><pre><code>import json\nwith open('ntuples.json', 'r') as f:\n    metadata = json.load(f)\n</code></pre> From this information, you can now calculate the event weight for each of the simulated samples. Our integrated luminosity for this 2015 data is 2256.38 pb. </p> <p>Question</p> <p>Write a function to calculate the event weight (<code>getEventWeight</code>) for a given simulated sample, using the metadata from <code>ntuples.json</code>.  </p> Show answer <pre><code>def getEventWeight(sample,file_index):\n    lumi = 2256.38 # inv pb \n    if sample != \"data\":\n        xsec_weight = metadata[sample]['filter_eff'] * metadata[sample]['xsec'] * lumi / metadata[sample]['nominal'][file_index]['nevts']\n    else:\n        xsec_weight = 1\n    return xsec_weight\n</code></pre> <p>Finally, we want to save our histogram in a format that can be interpreted by the <code>Combine</code> tool that we use for statistical analyses in CMS. There are many different formats that can be used, however we will use a simple <code>.csv</code> format for these exercises. </p> <p>I have created a simple conversion function for you to use in the file <code>hist2df.py</code>. </p> <p>You can convert a histogram created with the matplotlib <code>hist</code> function to a dataframe using this function as follows, </p> <pre><code>weights =  plt.hist(observable,bins=np.arange(50,575,25))\ndfs = histogramToDataframe(weights[0],\"signalregion\",\"ttbar\")\n</code></pre> <p>We also want to save our histograms for the case where we shifted the jet energy scales (jes) up and down - eg,  <pre><code>weightsUp =  plt.hist(observableUp,bins=np.arange(50,575,25))\ndfsUp = histogramToDataframe(weightsUp[0],\"signalregion\",\"ttbar\",sys=\"jesUp\")\n</code></pre></p> <p>Since we haven't included the event weights when making the histogram, we will multiply the column <code>sum_w</code> by this weight.  <pre><code>event_weight  = getEventWeight('ttbar',0) # the weight needed when using only the first file \ndfs['sum_w']  *= event_weight \ndfs['sum_ww'] *= event_weight*event_weight\n</code></pre></p> <p>Note that there is also a column in our dataframe called <code>sum_ww</code> that we have multiplied by the square of the event weight. This column is the variance of the bin contents due to the limited number of MC simulated events available. We'll discuss more about this column in later exercises. </p> <p>Finally, we can save the histogram in the format needed later for <code>Combine</code> using,  <pre><code>dfs = df.sort_values(by=['channel','process','systematic','bin'])\ndfs.to_csv('histograms.csv',index=False)\n</code></pre> The first line is just to avoid some python warnings later on. You'll now see a file called <code>histograms.csv</code> in your file browser. </p>"},{"location":"day1/#full-analysis","title":"Full analysis","text":"<p>In this exercise, we only ran our selection and created a histogram for one of the files for one of our simulated processes. We should run the same selection over all of the simulated files and for the data too!</p> <p>Question</p> <p>Write some code that will run over all of the simulated sample files and all of the data files too. Remember, </p> <ol> <li>Only look at the samples called <code>nominal</code> in the <code>.json</code> file.  </li> <li>Make a separate histogram for each of the different processes and for the data. Each simulated sample will have its own event weight that you'll need to calculate. </li> <li>Remember to include a histogram for the up and down variations of the jet energy scale for every sample except the data.  </li> <li>You can save all of the histograms to the same <code>.csv</code> file or you can save them in separate files.</li> </ol> <p>This may take some time so you should get some working code and then you may need to let the commands run for a couple of hours - maybe leave it running over dinner or even overnight.  Remember, if you don't access the full set of files, you should recalculate the integrated luminosity (you can assume that the new integrated luminosity scales with the number of events accessed) and modify the function to calculate the event weight to account for missing simulated events. </p> <p>Feel free to choose your own binning for your observable or you could even choose a different observable to the one we calculated in the example above! You might even try saving different histograms with different binnings/observables so that you can see what difference this makes for the statistical analysis later. </p> <p>Please try to write your own code before looking at the answer provided. </p> Show answer <p>I have uploaded a Jupyter notebook that will perform the full analysis on the samples called <code>FullAnalysis.ipynb</code>in the <code>ttbarAnalysis</code> folder. If you really get stuck, have a look at this notebook to see how to run our object and event selection, calculate the observable and save the histograms for all of the samples. </p> <p>The notebook will produce a plot similar to the one below,</p> <p></p>"},{"location":"day2/","title":"Exercise 2 - Maximum Likelihood Fits","text":"<p>From yesterday's exercise, we now have a set of histograms, from data and simulation, in our <code>.csv</code> file. Don't worry if you didn't manage to run over all of the samples from yesterday, you can use the pre-prepared file <code>ttbarAnalysis/exercise1solutions/signalregion_mbjj.csv</code>.</p> <p>In today's exercises, we're going to use the CMS statistics software tool <code>combine</code> to extract statistical results from the data (and simulation) that we processed yesterday. <code>combine</code> is a software package that is designed with a command line interface that uses simple <code>.txt</code> files as inputs. You can find out lots more about the tool at the online documentation pages here.</p> <p>We'll begin by starting the container that has <code>combine</code> compiled for us. If you didn't download the container already, go back to the Getting started pages before continuing. </p> <p>To do this, type the following into a terminal on your laptop (or by clicking the play button next to the <code>cms_combine</code> container in the Docker desktop application and using the terminal there).</p> <pre><code>docker start -i cms_combine\n</code></pre> <p>We'll also need to checkout the GitHub area since this is the first time we're using this container.</p> <pre><code>git clone https://github.com/nucleosynthesis/LHCDataStatisticsICISE2024.git\ncd LHCDataStatisticsICISE2024/ttbarAnalysis \n</code></pre> <p>Now that we're inside the container, start jupyter lab and enter the URL that gets printed to the screen in your preferred browser. </p> <pre><code>jupyter lab --ip 0.0.0.0 --port 8889 --no-browser\n</code></pre>"},{"location":"day2/#datacards","title":"Datacards","text":"<p>Create a new Text file and give it a name (I called mine <code>signalregion_mbjj.txt</code>). This text file will be the input to our <code>combine</code> commands - we call it a Datacard. Copy the text below into your text file. </p> <pre><code>imax 1\njmax 4\nkmax 0\n# -----------------------------------------------------------------------------------------\nshapes data_obs signalregion signalregion_mbjj.csv signalregion:data:nominal,sum_w:sum_ww\nshapes *        signalregion signalregion_mbjj.csv signalregion:$PROCESS:nominal,sum_w:sum_ww \n# -----------------------------------------------------------------------------------------\nbin         signalregion\nobservation -1\n# -----------------------------------------------------------------------------------------\nbin         signalregion  signalregion        signalregion       signalregion   signalregion\nprocess     ttbar         single_atop_t_chan  single_top_t_chan  single_top_tW  wjets\nprocess     0             1                   2                  3              4\nrate        -1            -1                  -1                 -1             -1\n# -----------------------------------------------------------------------------------------\n</code></pre> <p>Let's go through these lines and see what they are doing. </p> <p>The first lines, specifying <code>imax</code>, <code>jmax</code> and <code>kmax</code> indicate the number of channels, backgrounds and systematic uncertainties, respectively. In our case, we only have one channel (the signal region), and we have four background processes. Right now, we don't have any systematic uncertainties included but later on, we'll add some - for now, we keep this as 0. </p> <p>The next lines (starting with <code>shapes</code>) say where <code>combine</code> will find the distributions (histograms) for the data and the different processes. The first line,  <pre><code>shapes data_obs signalregion signalregion_mbjj.csv signalregion:data:nominal,sum_w:sum_ww\n</code></pre> says that the shape (distribution or histogram if you like) for the observed data can be found in our <code>signalregion_mbjj.csv</code> file in the rows where <code>process==data</code> and <code>systematic==nomial</code>. The final parts of the line say that the column <code>sum_w</code> represents the number of events in each bin. For the data <code>sum_ww</code> isn't used but this part of the line is needed for the code to work. </p> <p>The next line is very similar  <pre><code>shapes *        signalregion signalregion_mbjj.csv signalregion:$PROCESS:nominal,sum_w:sum_ww \n</code></pre> This line is telling <code>combine</code> where to find the shapes for the signal and background processes. Here the <code>*</code> means that this applies for all of the processes. The keyword <code>$PROCESS</code> is expanded for each process that we define in the datacard. This is useful to avoid having to write the same datacard line over and over. </p> <p>Next, we have a line that indicates the total numbers of events for the observed data and the different processes in our signal region.  <pre><code>bin         signalregion\nobservation -1\n</code></pre> This tells <code>combine</code> that there is a channel called <code>signalregion</code>. The observation is usually a number but the <code>-1</code> there tells <code>combine</code> to go and calculate the total number of observed events by summing up the number of events in each bin - this saves us having to write the number each time.  The next lines are similar,  <pre><code>bin         signalregion  signalregion        signalregion       signalregion   signalregion\nprocess     ttbar         single_atop_t_chan  single_top_t_chan  single_top_tW  wjets\nprocess     0             1                   2                  3              4\nrate        -1            -1                  -1                 -1             -1\n</code></pre></p> <p>This tells <code>combine</code> which processes are expected to contribution to the signal region. The names given should match the names in our <code>.csv</code> file so that the <code>shapes</code> line above will find the right histograms. Again, we use <code>-1</code> for the rate so that <code>combine</code> does the calculating for us. Finally, we also give a process ID. Any process with a value \\leq will be considered a signal process, while any process with a value &gt;0 will be a background process. This is important later when we perform statistical tests. </p>"},{"location":"day2/#performing-a-first-fit","title":"Performing a first fit","text":"<p>Now that we have our Datacard written. We can perform some statistical calculations. We can perform a fit to the data, allowing the total rate of the <code>ttbar</code> process to vary. <code>combine</code> will automatically create a signal strength parameter - <code>r</code> which multiplies the rate of any signal process defined in the datacard. </p> <p>We will use the <code>FitDiagnostics</code> method to extract the best fit value for the parameter <code>r</code> given the data we observed. We do this by typing the following command in a terminal (you can open a terminal in  jupyter lab), </p> <pre><code>combine signalregion_mbjj.txt -M FitDiagnostics\n</code></pre> <p>You should get something similar to the output below  <pre><code> --- FitDiagnostics ---\nBest fit r: 0.903186  -0.00366585/+0.00367476  (68% CL)\nDone in 0.01 min (cpu), 0.01 min (real)\n</code></pre> You will likely get some warnings about <code>PDF didn't factorize!</code> that you can safely ignore for now.</p> <p>So it looks like the fit would prefer a value of the tt cross-section around 10% smaller than the predicted value that we used when we processed the ttbar sample. You may have a different result if you are using your own <code>.csv</code> file. </p> <p>Note that you also will now have files called <code>higgsCombineTest.FitDiagnostics.mH120.root</code> and <code>fitDiagnosticsTest.root</code> that got created from the above command. These files save the result in various ROOT object formats. We will use these  files later on, but for now you can just ignore this file. </p> <p>Let's see if our data/simulation agreement has improved after the fit. We can do this by asking combine to create new templates after the fit. We run the command again, but this time, we will add options to save templates with uncertainties to the <code>fitDiagnosticsTest.root</code> file. </p> <pre><code>combine signalregion_mbjj.txt -M FitDiagnostics --saveShapes --saveWithUncert\n</code></pre> <p>If you are unfamiliar with ROOT, I have created a file called <code>root2py.py</code> with some handy functions to read and convert the objects into simple python arrays that can be used for plotting with matplotlib. Below is an example (written in pyROOT or just python) to make a plot of the distributions after the fit. You can copy either of these </p> pythonpyROOT <pre><code>from root2py import *\nimport ROOT \n\nfile   = ROOT.TFile.Open(\"fitDiagnosticsTest.root\")\nfolder = file.Get(\"shapes_fit_s/signalregion\")\n\nresults = getHistogramCountsAndData(folder)\n\nimport matplotlib.pyplot as plt\n\nnsamples    = len(results['samples'])\nbin_centres = results['data'][0]\nbins_list   = [results['samples'][i][1][0][0:-1] for i in range(nsamples)]\nbin_boundaries = results['samples'][0][1][0]\nsamples_stack = [results['samples'][i][1][1] for i in range(nsamples)]\nlabels        = [results['samples'][i][0] for i in range(nsamples)]\ndata_errs     = [results['data'][2],results['data'][3]]\n\nplt.hist(bins_list,bins=bin_boundaries,weights=samples_stack,label=labels,stacked=True,density=False)\nplt.errorbar(bin_centres,results['data'][1]\n             ,yerr=data_errs\n             ,label='data'\n             ,marker='o'\n             ,markersize=4.0\n             ,color='black'\n             ,linestyle=\"none\")\n\n# calculate min and max from total errors on prediction for filled area\n# we need to add left most and right most bin edge for this to work \n\ntotalED = results['total'][1]-results['total'][2]\ntotalEU = results['total'][1]+results['total'][2]\n\ntotalED = np.insert(totalED,0,totalED[0])\ntotalED = np.append(totalED,totalED[-1])\n\ntotalEU = np.insert(totalEU,0,totalEU[0])\ntotalEU = np.append(totalEU,totalEU[-1])\n\nbin_centres_x = np.insert(bin_centres,0,0)\nbin_centres_x = np.append(bin_centres_x,len(bin_centres))\n\nplt.fill_between(bin_centres_x,totalED,totalEU,color='gray',alpha=0.25,step='mid')\nplt.ylim(0,1.3e4)\nplt.legend()\n</code></pre> <pre><code>import ROOT\n\ncanvas = ROOT.TCanvas()\n\nfile   = ROOT.TFile.Open(\"fitDiagnosticsTest.root\")\nfolder = file.Get(\"shapes_fit_s/signalregion\")\n\ndata  = folder.Get(\"data\")\nprint(data.GetN())\ntotal = folder.Get(\"total\"); total.SetFillColor(ROOT.kGray)\n\nttbar = folder.Get(\"ttbar\") ; ttbar.SetFillColor(ROOT.kOrange) ; ttbar.SetFillStyle(1001)\nwjets = folder.Get(\"wjets\") ; wjets.SetFillColor(ROOT.kRed+1)\nsingle_top_tW = folder.Get(\"single_top_tW\") ; single_top_tW.SetFillColor(ROOT.kBlue)\nsingle_atop_t_chan = folder.Get(\"single_atop_t_chan\"); single_atop_t_chan.SetFillColor(ROOT.kBlue-3)\nsingle_top_t_chan  = folder.Get(\"single_top_t_chan\"); single_top_t_chan.SetFillColor(ROOT.kBlue-9)\n\n# create legend and stack and fill them\nlegend = ROOT.TLegend(0.6,0.6,0.89,0.89)\nlegend.AddEntry(data,\"data\",\"pe\")\n\nstk = ROOT.THStack(\"stack\",\";;Events\")\nstk.Add(single_top_t_chan); legend.AddEntry(single_top_t_chan,\"single_top_t_chan\",\"F\")\nstk.Add(single_atop_t_chan); legend.AddEntry(single_atop_t_chan,\"single_atop_t_chan\",\"F\")\nstk.Add(single_top_tW); legend.AddEntry(single_top_tW,\"single_top_tW\",\"F\")\nstk.Add(wjets); legend.AddEntry(wjets,\"wjets\",\"F\")\nstk.Add(ttbar); legend.AddEntry(ttbar,\"ttbar\",\"F\")\n\n#data.Draw(\"ApE\")\nstk.Draw(\"hist\")\ndata.Draw(\"pE\")\n#ttbar.Draw(\"histsame\")\ntotal.Draw(\"E2same\")\nlegend.Draw()\ncanvas.Draw()\n</code></pre> <p>Challenge</p> <p>Add a ratio panel below the main plot that shows data/total expected. This can be a good way to judge more easily by eye how well the data and predictions agree after the fit. </p>"},{"location":"day2/#adding-additional-rate-modifiers","title":"Adding additional rate modifiers","text":"<p>The agreement looks a little better than before but we could still do better. We should also allow our dominant background process (<code>wjets</code>) to float so that the fit can adjust the total rate. </p> <p>Add the following line to your Datacard,  <pre><code>wjets_norm rateParam signalregion wjets 1 [0,5]\n</code></pre></p> <p>This line tells combine to add a new parameter to the model that multiplies the rate of the specified process, in the specified channel. The new parameters will be <code>wjets_norm</code>, which multiplies the rate of the <code>wjets</code> process. We've chosen the default value of the parameter to be 1 and allowed its range to vary between 0 and 5. </p> <p>Question</p> <p>Run the <code>-M FitDiagnostics</code> method again and plot the new best fit shapes. Does the agreement improve compared to before? What are the values of the fitted rate multipliers (add the option <code>-v 3</code> to get combine to print this information).</p> Show answer <p>By running <pre><code>combine signalregion_mbjj.txt -M FitDiagnostics -v 3 --saveShapes --saveWithUncert\n</code></pre> we now find in the output  <pre><code>r          = 0.824465     +/-  0.0115119 (limited)\nwjets_norm = 2.45416      +/-  0.203045  (limited)\n</code></pre> Now we see that the fit would prefer to scale up the <code>wjets</code> process by a factor of ~2.45 compared to our original prediction. The best-fit value for the signal strength has also changed from what we saw before. </p> <p>If we look at a plot of the fit results we can see things are a little improved. The plot below is made using the pyROOT version of our plotting code. </p> <p></p> <p>Remember, your results might be different if you're using your own solution from the first exercise. </p> <p>You may have noticed that in addition to the best fit value, <code>combine</code> reports an uncertainty <code>+/-</code>. We will cover what we mean by uncertainty in the lectures, but this uncertainty is estimated by using the inverse of the Hessian matrix from the fit result. From this alone, we can see that by including an additional parameter, the uncertainty on our best fit for <code>r</code> has increased. Generally, we expect that adding freedom to the fit will increase the uncertainty of our results but improve the overall agreement in our data/simulation plots.  </p> <p>We can estimate this uncertainty more accurately by scanning the profiled log-likelihood and comparing this for the two cases; with and without the <code>wjets_norm</code> parameter. </p>"},{"location":"day2/#likelihood-scans","title":"Likelihood scans","text":"<p>Remember in the lectures, we define a quantity q(r) as the following, </p>  q(r) = -2\\ln \\frac{L(r,\\hat{\\nu}_{r})}{L(\\hat{r},\\hat{\\nu})}  <p>where the \\hat{\\cdot} notation means the maximum likelihood estimate (or best-fit) and the subscript and where \\nu represents our nuisance parameters. In this case, we only have one such nuisance parameter which is <code>wjets_norm</code>. The value \\hat{\\nu}_{r} is the value of \\nu for that maximises the likelihood when r is fixed to a specific value - sometimes we call this the conditional maximum likelihood. </p> <p><code>combine</code> can calculate this quantity as a function of our parameter r using the <code>-M MultiDimFit</code> method and the <code>grid</code> algo with the command, </p> <pre><code>combine signalregion_mbjj.txt -M MultiDimFit --algo grid --points 50 --setParameterRanges r=0.7,1\n</code></pre> <p>The option <code>--points</code> tells combine how many evenly spaced points at which to evaluate the function q(r) and the last option specifies the range that those points should be in. After running this command, there will be a new ROOT file called <code>higgsCombineTest.MultiDimFit.mH120.root</code> in your file browser. </p> <p>Warning</p> <p>If you run the <code>combine</code> command twice, the output file will be overwritten. By adding the option <code>-n name</code>, you can avoid this as the word <code>Test</code> will be modified to <code>name</code> that you specify. Use this to keep track of different results.  </p> <p>This file contains a <code>TTree</code> that stores the values of r and 0.5\\times q(r) in its branches. In the python file <code>root2py.py</code> I have included a function to convert these into python arrays for you. You can plot the value of q(r) using one of the blocks of code below, depending on if you prefer pyROOT or straight python. </p> pythonpyROOT <pre><code>from root2py import *\nimport ROOT\nfscan = ROOT.TFile(\"higgsCombineTest.MultiDimFit.mH120.root\")\ntree  = fscan.Get(\"limit\")\nx,y = get2DeltaNLLScan(tree)\nplt.plot(x,y) \n</code></pre> <pre><code>import ROOT\nfscan = ROOT.TFile(\"higgsCombineTest.MultiDimFit.mH120.root\")\ntree  = fscan.Get(\"limit\")\ncanvas = ROOT.TCanvas()\ntree.Draw(\"2*deltaNLL:r\",\"\",\"L\")\ncanvas.Draw()\n</code></pre> <p>Question</p> <p>Make a plot of q(r) for the case where the rate of <code>wjets</code> is allowed to float and where it is fixed to the original value, on the same axis. Which one is wider? What does that tell us?</p> Show answer <p>You should find something like the plot below by using the code provided above and modifying to plot two scans on top of one another. </p> <p></p> <p>The scan where the rate of <code>wjets</code> is allowed to float is much wider than the original scan. </p> <p>Challenge</p> <p>To estimate the uncertainty, we can define confidence intervals, according to Wilks' theorem, as the region for which q(r)&lt; x_{\\alpha} where \\alpha is our test size. For 1-\\alpha=0.683, i.e a 68.3% confidence interval, x_{\\alpha}=1. Find the 68.3% intervals for each of the scans using this method. </p>"},{"location":"day2/#goodness-of-fit","title":"Goodness-of-fit","text":"<p>Instead of comparing our data/simulation plots by eye, we can use a measure of the Goodness-of-fit. In lectures we learned about hypothesis testing. <code>combine</code> has several routines for calculating a goodness-of-fit, but we will just use the so called <code>saturated</code> test for an idea of the fit quality. </p> <p>The test-statistic for a template model is, </p> <p>$$     t = -2\\ln\\frac{L(\\hat{r},\\hat{\\theta})}{L_{s}} $$ where L_{s} is the saturated likelihood. It is defined as the likelihood valye where the expected value in each bin is exactly the same as the observed value - i.e the very largest possible likelihood value given the data observed. </p> <p>We can calculate this value with <code>combine</code> using the following,  <pre><code>combine signalregion_mbjj.txt -M GoodnessOfFit --algo=saturated\n</code></pre></p> <p>A result in the literature (Wilks' theorem) tells us that for large statistics samples (in the asymptotic limit), this test statistic will be distributed according to a \\chi^{2} distribution with n degrees of freedom where n is the number of bins in the distribution minus the number of parameters in the fit - in our case this is n=20-2=18. This means, we can convert the number calculated by combine to a p-value using, </p>  p = \\int_{t}^{+\\infty} \\chi^{2}(n)dt   <p>or in code form using the function below  <pre><code>import ROOT \nROOT.TMath.Prob(t,18)\n</code></pre></p> <p>Question</p> <p>Calculate the p-value for both cases (with and without the <code>wjets_norm</code> parameter). Which one is better? </p> Show answer <p>You should find a very very small value in both cases so both fits are pretty awful. </p> <p>Challenge</p> <p>In the <code>combine</code> online documentation, on Goodness of fit tests, you can find instructions for calculating p-values using toys rather than relying on asymptotic methods. Calculate the p-values for the two cases using the saturated algorithm.</p>"},{"location":"day2/#control-regions","title":"Control regions","text":"<p>In the previous examples we saw that we can trade between sensitivity in our measurement (the uncertainty in <code>r</code>) and quality of the fit result. In an ideal world, we would have a good overall quality of the fit and a reasonably small uncertainty in our results. </p> <p>We saw that the fit improved when allowing the rate of <code>wjets</code> to float in the fit, but at the cost of increased uncertainty in <code>r</code>. We can improve this by constraining the parameter <code>wjets_norm</code> using a control region. </p> <p>A control region is another region in the data that is dominated by a particular background process. By including this region in the fit, we will be able to use that additional data to constrain the <code>wjets_norm</code> parameter. </p> <p>Let's go back to the <code>cms_python</code> container and modify our analysis notebook to create a control region that will be dominated by the <code>wjets</code> process. If you didn't manage to get a working analysis yesterday, you can use the notebook provided <code>ttbarAnalysis/exercise1solutions/FullAnalysis.ipynb</code>. </p> <p>We need to change how we are filtering our events, we want to modify the selection so that we choose events with no b-tagged jets but everything else is the same, i.e change,   </p> <p><pre><code>(ak.sum(selected_jets.btag &gt;= 0.8, axis=1) &gt;= 2)\n</code></pre> to  <pre><code>(ak.sum(selected_jets.btag &gt;= 0.8, axis=1) &lt; 1)\n</code></pre></p> <p>Our observable will be different now since we no longer want to pick the trijet with a b-quark. Instead we just use the trijet with the largest p_{T}, i.e we change the observable calcualtion to  <pre><code>trijet = ak.combinations(selected_events_CR, 3, fields=[\"j1\", \"j2\", \"j3\"])\nj1,j2,j3 = ak.unzip(trijet)\ntrijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3\ntrijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\nobservable = ak.flatten(trijet_mass).to_numpy()\n</code></pre> for the control region. We will use this control region in combination with our signal region in tomorrow's exercise. </p> <p>Question</p> <p>Write a new notebook to process the events but this time applying the control region selection.  If you prefer, you can instead modify your original notebook so that it can process the events for the signal region and control region at the same time! Remember,     - Also modify the name of the output file to something like <code>controlregion_mbjj.csv</code> so as not to overwrite our signal region.     - You should name the channel for the control region something like \"<code>controlregion</code>\" as we'll use this name for our datacards later - remember this when using the <code>histogramToDataframe</code> function.     - We also want to include the histograms for the jes up and down variations just like we did in the control region. </p> Show answer <p>I have uploaded a Jupyter notebook that will perform the  analysis on the samples and produce both the signal region and control region in a single output <code>.csv</code> called <code>allregions.csv</code>. The notebook os called <code>FullAnalysisWithControlRegion.ipynb</code>in the <code>ttbarAnalysis</code> folder. If you really get stuck, have a look at this notebook to see how to run our object and event selection, calculate the observable and save the histograms for all of the samples this time with the control region. </p> <p>The distributions in the control region should look something like the plot below </p> <p></p>"},{"location":"day3/","title":"Day3","text":"<p>Challenge</p> <p>In the online documentation, on Goodness of fit tests, you can find instructions for calculating p-values using toys rather than relying on asymptotic methods. Calculate the p-values for the two cases using the saturated algorithm.</p>"},{"location":"day3/#information-for-signal-search","title":"Information for signal search","text":"<p>events = NanoEventsFactory.from_root(     \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/RunIIFall15MiniAODv2_TprimeTprime_M-1200_TuneCUETP8M1_13TeV-madgraph-pythia8_flat/5A744C3D-EA4C-4C35-9738-BF878E063562_flat.root\",     , schemaclass=AGCSchema, treepath='events').events()</p> <p>filter_eff = 1  xsex = 0.0118 # pb for 1200 GeV Tprime  nevts_blah = 253993</p> <p>obsbins=np.arange(500,2050,80)</p> <p>lumi uncertainty in 2015 - 2.2% </p>"},{"location":"setup/","title":"Setup","text":"<p>To complete these exercises, we will be using two container images, with the software installed for you. In the examples here, we will use docker to run the images. The docker desktop is available for mac, windows and linux so follow the link and download the right installation for your personal laptop. </p> <p>Once you have the docker desktop installed, make sure it is running and get the two containers that we'll need for the exercises using the terminal commands below. Note that the docker desktop has its own terminal if you prefer to use that one. </p>"},{"location":"setup/#python-environment-for-cms-open-data-datasets","title":"Python environment for CMS Open Data datasets","text":"<p>Obtain the <code>cms_python</code> container using, </p> <pre><code>docker run -it --name cms_python -P -p 8888:8888 -v ${HOME}/cms_open_data_python:/code gitlab-registry.cern.ch/cms-cloud/python-vnc:python3.10.5\n</code></pre> <p>Now that you're inside the container, run the following to get all of the necessary scripts and install some additional packages.  <pre><code>pip install vector coffea==0.7.21\n</code></pre></p> <p>You can exit the container at any time by typing <code>exit</code> in the terminal. To restart the python container, open a terminal and enter  <pre><code>docker start -i cms_python\n</code></pre></p>"},{"location":"setup/#combine-package-for-statistical-analysis","title":"Combine package for statistical analysis","text":"<p>Obtain the <code>cms_combine</code> container using, </p> <pre><code>docker run -p 127.0.0.1:8889:8889 --name cms_combine -it gitlab-registry.cern.ch/cms-cloud/combine-standalone:v9.2.1-slim\n</code></pre> <p>If you like to make plots with python instead of using ROOT, then you should also install <code>matplotlib</code> inside this container by running the following command in the terminal inside the container.  <pre><code>pip install matplotlib \n</code></pre></p> <p>You can exit the container at any time by typing <code>exit</code> in the terminal.  To restart the combine container, open a terminal and enter  <pre><code>docker start -i cms_combine\n</code></pre></p>"},{"location":"setup/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>As much as possible, we will use Juptyer notebooks to write and run code for these exercises. You can launch JupterLab from either of the two containers by using the following command inside the terminal after starting the container. </p> <pre><code>jupyter lab --ip 0.0.0.0 --port XXXX --no-browser\n</code></pre> <p>where <code>XXXX</code> should be <code>8888</code> for the python container, and <code>8889</code> for the combine container. </p> <p>The output will give you a link that you can paste in your preferred internet browser to open the JupyterLab environment. You should see something like the following for example when launching JupyterLab from the combine container, </p> <p></p> <p>From here, we can open a new terminal, text file or notebook. On the left you can see a file browser that shows all of the files contained in the container. You can modify/copy/delete these using the file browser by right clicking on them.</p> <p>Since our data structures will be mostly Pandas dataframes, you might want to install the Jupyter spreadsheet editor that allows you to both view and edit CSV files,  <pre><code>pip install jupyterlab-spreadsheet-editor\n</code></pre> You can do this in both of the containers but you don't need to install this for the exercises. </p>"},{"location":"setup/#moving-files-around","title":"Moving files around","text":"<p>From time to time, we will need to move files between containers or to our own computers. You can do this by downloading the file from the browser (right click on a file in the file browser and select \"downlowd\") or you can use the <code>docker cp</code> tool. For example to transfer a file called \"myfile.txt\" from your local desktop to the <code>cms_python</code> container, you can run, </p> <pre><code>docker cp ~/Desktop/myfile.txt cms_python:/code/\n</code></pre> <p>in a terminal on your local machine. You can also copy files from the containers to your local machine by reversing the order of the locations in the command above. </p>"}]}