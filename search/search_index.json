{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"INTRODUCTION","text":"<p>This is the webpage for the lecture series on Data Analysis and Statistics at the LHC. In these pages, you can find links to the lectures as well as exercises for you to explore a typical data analysis at CMS.  You should be familiar with simple programming in Python, however, the examples should give you enough information to produce the desired output even if you don't know much Python. </p> <p>There are four sets of exercises in these pages that you can find under the Exercises tab, one for each day of the course. You should read through the information given and copy code/commands where prompted into your own terminal or notebook. </p> <p>Question</p> <p>Throughout the exercises, there will be questions in a green box like this that contains a challenge for you to try on your own. Below, there will be a solution in a blue box for you to check against your own answer or in case you get really stuck. Please do try on your own before revealing the solution. </p> Show answer The answer will be shown here  <p>Challenge</p> <p>If you are feeling especially confident, you can have a go at extra challenges that I have put in these turquoise boxes. I won't provide a solution for these challenges, but you don't need to complete them to progress through the other exercises. </p> <p>Warning</p> <p>Sometimes, there will be some technical or conceptual hurdle that requires a bit of careful thought to work around. I have tried to point these out to you in orange boxes like this one to help avoid any major road blocks. </p> <p>Throughout the exercise, you will see code blocks like the ones below. Those that are labelled as Text only are either output from some command or they are text that you can copy (eg Datacards), </p> Text Only<pre><code>This is just simple text\n</code></pre> <p>Those labeled with Bash, indicate commands that can be typed into your Terminal and executed there,  Bash<pre><code>echo \"Hello! I am in a bash terminal\"\n</code></pre></p> <p>and those with Python as the label indicate python code that can be run inside a Jupyter notebook Python<pre><code># This is python code\ndef printHello():\n    print(\"Hello!\")\nprintHello()\n</code></pre></p> <p>For all of these, there is a copy icon in the top right that you can use to copy the entire contents of the block. </p>"},{"location":"#getting-started-pre-course","title":"Getting started (Pre-Course)","text":"<p>We are going to need some software for the exercises. All of the software required is available pre-packaged in container images that you can run using Docker! Head over to the Getting Started pages now to install Docker and obtain the relevant images. This can take some time so please do this before the start of the lecture course. </p>"},{"location":"#useful-links","title":"Useful Links","text":"<p>You can find more useful information about the software we will be using below </p> <ul> <li>Docker: Docker is a desktop client for running the container images that we will be using for these exercises.  </li> <li>CMS Open Data: The CMS Collaboration regularly releases open datasets for public use. This link is the CMS Open Data Guide</li> <li>Combine: The Combine tool is the main statistical analysis software package used by the CMS Collaboration. It is based on the ROOT/RooFit software packages, however for these exercises, you do not need to be an expert user of these packages. </li> </ul>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Dr. Nicholas Wardle is a lecturer in Physics at Imperial College London. He is also a member of the CMS Collaboration and his research is focused on statistical methods for data analysis and searches for BSM physics through precision measurements of Higgs boson properties. </p>"},{"location":"day1/","title":"Exercise 1 - Datasets, Selections and Histograms","text":"<p>For the exercises, we will use a simplified version of a CMS analysis to measure the production cross-section of top-quark / anti-top-quark pairs - \\sigma_{tt} - in proton-proton collisions at a centre of mass energy of 13 TeV. We will be using real data from the CMS experiment, taken in 2015 during Run 2 of the LHC. The code is based on the CMS open data workshop linked here, which you can read through if you are interested. </p> <p>The image below is a Feynman diagram of the process we are interested in measuring, namely t\\bar{t}\\rightarrow (bW^{+})(\\bar{b}W^{-})\\rightarrow bl^{+}\\nu_{l}\\,\\bar{b}q\\bar{q} - the lepton+jets final state. </p> <p></p> <p>Of course, the charge conjugate version (with the sign of the charged lepton flipped) is also possible. In the following, we will consider both versions so don't get too hung up on the charges of the particles in the final state. We'll drop the charge and anti-particle symbols from here-on. </p> <p>In our example, we will focus on the case where the lepton l is an electron or muon.  In the CMS detector, these leptons are extremely well measured and are very good for \"triggering\" the events that we are interested in. Looking at the final state, we would also expect to see 4 hadronic jets, two of which will be b-jets, and missing transverse momentum from the neutrino which isn't detected.  </p>"},{"location":"day1/#launch-jupyterlab","title":"Launch JupyterLab","text":"<p>For this exercise, we will be using data from the LHC Run-2 proton-proton collision data recorded by CMS during 2015.  In addition to the data, we will be using simulated samples of tt\\rightarrow (bW)(bW)\\rightarrow bl\\nu_{l}\\,bqq production and various background processes that can mimic the signal.  </p> <p>First, start the <code>cms_python</code> container using  Text Only<pre><code>docker start -i cms_python\n</code></pre> Note that you should have the Docker desktop running on your laptop for this to work. If you didn't install the container already, please see the Getting started pages before continuing. </p> <p>Next, you should checkout the GitHub area that contains some helpful files and information for accessing and processing the data. In the same terminal as you started the container in type,  Text Only<pre><code>git clone https://github.com/nucleosynthesis/LHCDataStatisticsICISE2024.git\ncd LHCDataStatisticsICISE2024/ttbarAnalysis \n</code></pre></p> <p>Now you can start the JupyterLab server by running  Text Only<pre><code>jupyter lab --ip 0.0.0.0 --port 8888 --no-browser\n</code></pre></p> <p>The output will give you a link (starting with <code>http://</code>) that you can paste into your preferred internet browser to open the JuypterLab session. Create a new Python 3 notebook - icon that says Python 3 (ipykernel) - and give it a name by right clicking in the file browser at the left panel where it says \"Untitled.ipynb\". We will use this notebook to issue commands to process the data. </p> <p>You can create new cells in the notebook by using the \"+\" icon at the top. The code that is entered in each cell can be run by either clicking the play icon at the top or by selecting the cell and pressing Shift + Enter on your keyboard. Jupyter notebooks are a great way of editing code and displaying output all in one place! </p>"},{"location":"day1/#accessing-and-exploring-the-data-sets","title":"Accessing and exploring the data sets","text":"<p>In your JupyterLab file browser, you will see the list of files that were obtained from GitHub. There is a file called <code>ntuples.json</code> that contains the file paths for the different samples (data and simulation) that we'll use in these exercises. </p> <p>In the file browser, if you click on the <code>ntuple.json</code> file, it will open in the main panel. You can click on the different names to expand them and see the information contained in the file. It should look like the image below, </p> <p></p> <p>These datasets have been skimmed by applying certain selection criteria. This is to make the samples more manageable as the original datasets are TBs in size! The requirements applied are, </p> <ul> <li>That the events fired at least one of these triggers: <code>HLT_Ele22_eta2p1_WPLoose_Gsf</code>, <code>HLT_IsoMu20_v</code>, <code>HLT_IsoTkMu20_v</code>. These essentially require that the online selection (trigger) found at least one isolated electron or one isolated muon with a minimum p_{T}, in a certain part of the CMS detector (can you figure out from the names, what the p_{T} requirements at the online selection are?). </li> <li>That the event contains either at least one tight electron or at least one tight muon with p_{T}&gt; 26 GeV and |\\eta|&lt;2.1. When we use the word tight in this context, it is jargon internal to CMS that just specifies how pure we are trying to make the sample. A tight selection criteria would be quite pure and have fewer false positives than a medium or loose selection which might retain more true muons or electrons, but also have more false positives (fake electrons or muons). </li> </ul> <p>The data are saved as <code>.root</code> files. Don't worry if you are not familiar with the ROOT software, we will some very nice Python tools from the Hep Software Foundation to read these files and convert them into Python based formats. </p> <p>Copy the code below into a cell in your Jupyter notebook. This imports the various modules that we need to read and convert our data into Python formats. </p> Python<pre><code>import uproot\nimport numpy as np\nimport awkward as ak\nfrom coffea.nanoevents import NanoEventsFactory, BaseSchema\nfrom agc_schema import AGCSchema\n</code></pre> <p>Run the cell to execute the commands. You should see the cell number (eg <code>[1]</code>) become an asterisk (<code>[*]</code>) for a short while, which indicates that the cell is running. Once the commands have finished, the number will appear again. </p> <p>Next, let's open one of the files from the <code>ttbar</code> sample. Copy the command below into the next cell of your Jupyter notebook and run it. </p> Python<pre><code>events = NanoEventsFactory.from_root('root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/RunIIFall15MiniAODv2_TT_TuneCUETP8M1_13TeV-powheg-pythia8_flat/00EFE6B3-01FE-4FBF-ADCB-2921D5903E44_flat.root', schemaclass=AGCSchema, treepath='events').events()\n\nprint(events.fields)\n</code></pre> <p>Warning</p> <p>You may find that sometimes you will see an <code>IOError</code> due to XRootD failing to access the remote files. This can be annoying so I recommend that you split up the task across different cells and that you wrap the command to open the file with a <code>try</code> <code>except</code> block similar to,  Python<pre><code>input_file = \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/Run2015D_SingleMuon_flat/07FC2CD2-106C-4419-9260-12B3E271C345_flat.root\"\ntry: \n    events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\nexcept OSError:\n    time.sleep(2) # sleep for 2 seconds and try one more time\n    try: \n        events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\n    except OSError:\n        time.sleep(2) # sleep for 2 seconds just to not try EOS again but give up on this file now\n        return []\n</code></pre> This will try to access a file twice before giving up and returning an empty list.</p> <p>The output should be a list of the different collections (fields) that are contained in the data. Each event may have a different number of objects in each collection. We can see this by entering the following code in the Jupyter notebook cell, </p> Python<pre><code>print(\"Electrons:\")\nprint(f\"Number of events: {ak.num(events.electron, axis=0)}\")\nprint(ak.num(events.electron, axis=1))\nprint()\n</code></pre> <p>The output should look like  Text Only<pre><code>Electrons:\nNumber of events: 325519\n[3, 2, 2, 1, 1, 2, 0, 1, 2, 3, 1, 1, 3, 4, ... 4, 1, 1, 2, 5, 3, 1, 2, 2, 2, 1, 2, 1]\n</code></pre></p> <p>From this we can see that there are 325519 events in total in this file and each event has a different number of electrons. Notice that we used the same <code>ak.num</code> command in each printout but we changed the depth of the sum by specifying <code>axis=</code>. The larger this number, the deeper into the event structure the function goes. </p> <p>Question</p> <p>Do the same for <code>muon</code>, <code>jet</code> and <code>met</code> in each event, to see how many of these objects are contained in each event. Why do you get an error when using the function for <code>met</code>, the missing transverse momentum of the event?  </p> Show answer  Similarly to the case for electrons, we can use the following,  Python<pre><code>print(\"Muons:\")\nprint(f\"Number of events: {ak.num(events.muon, axis=0)}\")\nprint(ak.num(events.muon, axis=1))\nprint()\n\nprint(\"Jets:\")\nprint(f\"Number of events: {ak.num(events.jet, axis=0)}\")\nprint(ak.num(events.jet, axis=1))\n</code></pre>  Which yields the output,  Text Only<pre><code>Muons:\nNumber of events: 325519\n[1, 1, 1, 0, 6, 6, 3, 2, 1, 2, 2, 6, 4, 2, ... 2, 1, 1, 2, 0, 5, 2, 3, 2, 2, 2, 2, 1]\n\nJets:\nNumber of events: 325519\n[4, 5, 4, 5, 9, 4, 4, 8, 5, 9, 5, 4, 6, 6, ... 6, 2, 4, 3, 5, 9, 4, 7, 4, 4, 6, 5, 4]\n</code></pre>  When doing the same for <code>met</code>, we get an error  Text Only<pre><code>ValueError: 'axis' out of range for 'num'\n</code></pre>  The reason is that the <code>met</code> field is an event level variable meaning it is calculated only once per event."},{"location":"day1/#applying-a-basic-analysis-selection","title":"Applying a basic analysis selection","text":"<p>We will apply selections to the events to discard those that are unlikely to have arisen from our target (signal) process t\\bar{t}\\rightarrow (bW^{+})(bW^{-})\\rightarrow bq\\bar{q}bl^{-}\\bar{\\nu}_{l}. </p> <p>Looking at the process, its clear that we should select events that have one muon or one electron, along with at least four jets which are either from a light flavour quark or from a b-quark (q or b). </p> <p>Generally, when applying selections to events, we first think about object selection - i.e we only want to apply event selections using well identified objects, and then event selection where we really target the final state that we are looking for. </p>"},{"location":"day1/#object-selection","title":"Object selection","text":"<p>We can see which features each of our collections have by checking the <code>fields</code> available. Copy the following code into a Juptyer notebook cell,  Python<pre><code>print(\"Electron fields:\")\nprint(events.electron.fields)\nprint()\n</code></pre> This should give the following output, </p> Text Only<pre><code>Electron fields:\n['pt', 'px', 'py', 'pz', 'eta', 'phi', 'ch', 'iso', 'veto', 'isLoose', 'isMedium', 'isTight', 'dxy', 'dz', 'dxyError', 'dzError', 'ismvaLoose', 'ismvaTight', 'ip3d', 'sip3d', 'energy']\n</code></pre> <p>We can see that the electrons have fields corresponding to, </p> <ul> <li>The kinematics of the electron: transverse momentum <code>pt</code> (p_{T}), pseudo-rapidity <code>eta</code> (\\eta)</li> <li>Identification quality information : Whether the electron passes loose, medium or tight selection requirements : <code>isLoose</code>, <code>isMedium</code>, <code>isTight</code>. </li> <li>Other features related to the track-calorimeter matching and how well isolated the electron object is. </li> </ul> <p>We can apply requirements on the objects by applying masks to our event collections. For electrons, we will apply the following criteria to the electron fields, </p> <ul> <li>p_{T}&gt; 30 GeV, |\\eta|&lt;2.1</li> <li>passing the tight identification requirements </li> <li>3D impact parameter (<code>sip3d</code>) &lt; 4</li> </ul> <p>Copy the code below to determine which events would pass this selection,  Python<pre><code>selected_electrons = events.electron[(events.electron.pt &gt; 30) &amp; (abs(events.electron.eta)&lt;2.1) &amp; (events.electron.isTight == True) &amp; (events.electron.sip3d &lt; 4 )]\n</code></pre></p> <p>Question</p> <p>Create the same object masks for selected muons and selected jets based on the following criteria, </p> <p>For muons: </p> <ul> <li>p_{T}&gt; 30 GeV, |\\eta|&lt;2.1 </li> <li>passing the tight identification requirements  </li> <li>3D impact parameter (<code>sip3d</code>) &lt; 4</li> <li>Relative isolation requirement (<code>pfreliso04DBCorr</code>) &lt; 0.15.</li> </ul> <p>For jets: </p> <ul> <li>Corrected (<code>corrpt</code>) p_{T}&gt; 30 GeV and |\\eta|&lt;2.4  The corrected p_{T} is the one obtained after applying a number of calibrations to the jets. </li> </ul> Show answer Python<pre><code>selected_muons = events.muon[(events.muon.pt &gt; 30) &amp; (abs(events.muon.eta)&lt;2.1) &amp; (events.muon.isTight == True) &amp; (events.muon.sip3d &lt; 4) &amp; (events.muon.pfreliso04DBCorr &lt; 0.15)]\nselected_jets = events.jet[(events.jet.corrpt &gt; 30) &amp; (abs(events.jet.eta)&lt;2.4) ]\n</code></pre>"},{"location":"day1/#event-selection","title":"Event selection","text":"<p>Now we want to apply the event level filters to our events. To do this, we use the <code>ak.count</code> method to count the number of objects of each type that we require in our events. Looking at our final state, our event selection will be, </p> <ul> <li>Exactly one electron or muon that passes the electron/muon requirements </li> <li>At least 4 jets, at least two of which should be a b-jet. </li> </ul> <p>For the first requirement, copy the following code into a Jupyter notebook cell and run it,  Python<pre><code>event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n</code></pre> The <code>+</code> acts as a logical or so that we are filtering on events that have exactly one electron or exactly one muon. Note that we could use another field of the object but here we have chosen to use <code>pt</code>. Remember, we want to apply the filter on each electron or muon in the events so the sum needs to be applied to the first layer (<code>axis=1</code>). </p> <p>For the second requirement, we need to define how we tag a b-jet. We use the combined secondary vertex algorithm at CMS to do this. When this quantity is high, this implies that the jet originates from a vertex which is far from the pp interaction point, which indicates that the jet originates from a heavy quark with a long(ish) lifetime - i.e a b-quark. For our selection  we will define b-jets as those for which the  b-tagging variable <code>btag</code> is greater than or equal to 0.8. </p> <p>The following code, which you should copy into a cell in your Jupyter notebook, will include the second requirement into our event filters, </p> Python<pre><code>event_filters = event_filters &amp;  (ak.count(selected_jets.corrpt, axis=1) &gt;= 4) &amp; (ak.sum(selected_jets.btag &gt;= 0.8, axis=1) &gt;= 2)\n</code></pre> <p>Note that we haven't actually applied any of this selection to our events yet. If we check the length of our event filters, we can see that its still the same size as our events,  Python<pre><code>print(len(event_filters))\n</code></pre></p> <p>The reason is that this object now contains a boolean flag, one for each event, that indicates whether or not the event satisfies all of the criteria that we used to define the filter. To actually select the events that pass this filter, we just apply the filter to one of our selected object collections. For example, to obtain the collection of jets from all of the events that pass our selection, we can do, </p> Python<pre><code>selected_events = selected_jets[events_filter]\n</code></pre> <p>As a final step, we want to plot some observable in our events that can separate the signal from the various backgrounds. Choosing observables often requires a lot of thought as physicists as there are several things we want to consider </p> <ul> <li>The observable should have some separation between signal and background to make it easier to extract the process that we are interested in. </li> <li>The observable may be more or less susceptible to experimental effects such as detector resolution, pile-up or be theoretically more prone to divergences in the calculation (this can especially true in jet related observables). All of these tend to imply systematic uncertainties that can reduce the sensitivity. </li> <li>The observable may lend itself to allowing for data-driven estimates of distributions for different backgrounds. This won't be the case here, but this can often be a good motivation for choosing certain variables. </li> </ul> <p>For this analysis, our chosen observable will be the mass of the hadronically decaying top-quark m_{bjj}.  We can use the <code>ak.combinations</code> function to find all combinations of 3-jets (trijet) in the events. For each combination, we will calculate the four-momentum of the 3-jet system and require that at least one of the 3 jets in each trijet is b-tagged. </p> <p>Copy the code below into a Jupyter notebook cell and run it, </p> Python<pre><code>trijet = ak.combinations(selected_events, 3, fields=[\"j1\", \"j2\", \"j3\"])\n\n# Get the individual jets from each combination \nj1,j2,j3 = ak.unzip(trijet)\n# booleans for which combination has one of the 3 jets b-tagged\nhas_bjet = (ak.sum(j1.btag&gt;=0.8,axis=1)+ak.sum(j2.btag&gt;=0.8,axis=1)+ak.sum(j3.btag&gt;=0.8,axis=1)&gt;0)\n# apply the filter \ntrijet = trijet[has_bjet]\n\ntrijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3 \n</code></pre> <p>Note that in the last command, <code>p4</code> is a special field that has the space-time co-ordinates of a four-vector. This makes our calculations of things like the p_{T} or mass of the trijets very easy. Below, we find the invariant mass of the trijet that has the highest p_{T}. You can find out more about what you can do with these objects at this coffea page. </p> Python<pre><code>trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n</code></pre> <p>Question</p> <p>Create an observable for the p_{T} of the trijet with the largest p_{T}. </p> Show answer Python<pre><code>trijet_pt = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].pt\n</code></pre>"},{"location":"day1/#jet-energy-scale-uncertainties","title":"Jet energy scale uncertainties","text":"<p>At the LHC experiments like CMS, one of the most important uncertainties for any analysis that looks at jets in the final state is the jet energy scale uncertainty. Remember that jets are created by clustering different particles together to reconstruct the energy of outgoing quark or gluon in the hard event. This means that lots of calibration is needed for the different detector components to properly estimate this energy. These calibrations come with uncertainties. In CMS, there are many different sources of uncertainty that come under the label \"jet energy scale\" but for this exercise, we'll just assume it can be modelled as one uncertainty. </p> <p>Take a look again at the fields of the jets in the event,  Python<pre><code>print(events.jet.fields)\n</code></pre></p> <p>You will notice there are two fields called <code>corrptUp</code> and <code>corrptDown</code>. These are the transverse momentum of the jets if we assume that the calibrations of the jet energy scale is shifted up or down by 1 standard deviation of the jet energy scale calibration measurement - i.e resulting from our uncertainty in these calibrations. For all of the simulated samples (everything that isn't data), we should calculate the observable three times, one for the nominal value of the p_{T} of the jets and one each for these variations. </p> <p>To do this, we simply need to make two new fields in our <code>selected_events</code> jets that represent the scale-factor applied to the energy-momentum four-vector of the jets. For the \"Up\" variation, do this with,  Python<pre><code>selected_events['scaleUp'] = selected_events.corrptUp/selected_events.corrpt\n</code></pre> Now, we can scale the jets in our selected trijets by this scale-factor. We can use the <code>multiply</code> method to scale each component of the jet four-vectors by their <code>scaleUp</code> and use these new four-vectors to calculate the observable.  </p> Python<pre><code>trijet[\"p4Up\"]   = trijet.j1.multiply(trijet.j1.scaleUp) +trijet.j2.multiply(trijet.j2.scaleUp)+trijet.j3.multiply(trijet.j3.scaleUp) \ntrijet_massUp   = trijet[\"p4Up\"][ak.argmax(trijet.p4Up.pt, axis=1, keepdims=True)].mass\nobservableUp    = ak.flatten(trijet_massUp).to_numpy()\n</code></pre> <p>We can see that the values in the <code>observableUp</code> array are shifted compared to the nominal ones in the <code>observable</code> array,</p> <p>Python<pre><code>print(observable)\nprint(observableUp)\n</code></pre> and we'll see Text Only<pre><code>[ 500.74713  291.3941   535.3709  ...  642.0001   783.4518  1441.1604 ]\n[ 506.01013534  294.2633194   539.46490363 ...  648.24548484  789.74960418\n 1450.60702913]\n</code></pre></p> <p>Question</p> <p>Calculate the observable again but this time for the <code>scaleDown</code> variation. </p> Show answer <p>We just need to use the <code>corrptDown</code> field instead, i.e  Python<pre><code>selected_events['scaleDown'] = selected_events.corrptDown/selected_events.corrpt\n...\ntrijet[\"p4Down\"]   = trijet.j1.multiply(trijet.j1.scaleDown) +trijet.j2.multiply(trijet.j2.scaleDown)+trijet.j3.multiply(trijet.j3.scaleDown) \ntrijet_massDown   = trijet[\"p4Down\"][ak.argmax(trijet.p4Down.pt, axis=1, keepdims=True)].mass\nobservableDown    = ak.flatten(trijet_massDown).to_numpy()\n</code></pre></p>"},{"location":"day1/#histograms-and-saving-output","title":"Histograms and saving output","text":"<p>Now that we have our event observable, we can see what the distribution looks like for these events.  First, we need to remove the array structure of the events so that we can use the plotting tools from matplotlib. We have to import this module too of course,  Python<pre><code>import matplotlib.pyplot as plt\nobservable = ak.flatten(trijet_mass).to_numpy()\nplt.hist(observable,bins=np.arange(50,575,25))\n</code></pre></p> <p>You should see a plot similar to the one below,  </p> <p>We see a peak close to the top mass (~175 GeV) as expected! The resolution of jets in the CMS detector is not very high so we do not see a very sharp peak, but we are clearly seeing the hadronic top quark here. </p> <p>Question</p> <p>Make a plot of the p_{T} of the trijet instead of the mass. You should be careful to choose appropriate binning for this variable. </p> Show answer Python<pre><code>plt.hist(observablept,bins=np.arange(0,300,25))\n</code></pre> <p>Challenge</p> <p>In the original CMS paper, the observable used was the invariant mass of the b-jet and the lepton (electron or muon) - m_{bl}. For the case where there is more than 1 b-jet in particular the minimum such mass from all pairings of b-jet with the lepton is used. Create this observable for the selected events and plot a histogram of it for our ttbar sample. </p> <p>As this sample is simulation, we need to apply proper event weights to account for the cross-section in the standard model. Remember that the number of events expected is given by the product of the cross-section and the integrated luminosity. For simulated samples, if we know the cross-section \\sigma and the number of events generated N_{gen}, we can calculate the effective luminosity of the simulated sample as, </p>  L_{eff}  =  N_{gen}/\\sigma  <p>Often, there is a filtering step in either the MC generation or in the skimming step - we don't always manage to process all of the samples. It's important to account for this by scaling up the number of generated events by this filter efficiency \\epsilon. So our formula changes to </p>  L_{eff}  =  \\frac{1}{\\epsilon}\\cdot N_{gen}/\\sigma  <p>The event weight for a particular sample will therefore be the ratio of the integrated luminosity of our real data to the effective luminosity of the sample, </p>  w = L_{int}/L_{eff} =L_{int}\\cdot \\epsilon\\cdot \\sigma/N_{gen}   <p>Have a look again inside the <code>ntuples.json</code> file. You will find that the file contains the number of events for each file in each sample - <code>nevts</code> -  (this is the number generated and processed to make that file), the filter efficiency for the sample (<code>filter_eff</code>) and the cross-section in pb - <code>xsec</code> for that sample. You can read in this data using the code below, </p> <p>Python<pre><code>import json\nwith open('ntuples.json', 'r') as f:\n    metadata = json.load(f)\n</code></pre> From this information, you can now calculate the event weight for each of the simulated samples. Our integrated luminosity for this 2015 data is 2256.38 pb. </p> <p>Question</p> <p>Write a function to calculate the event weight (<code>getEventWeight</code>) for a given simulated sample, using the metadata from <code>ntuples.json</code>.  </p> Show answer Python<pre><code>def getEventWeight(sample,file_index):\n    lumi = 2256.38 # inv pb \n    if sample != \"data\":\n        xsec_weight = metadata[sample]['filter_eff'] * metadata[sample]['xsec'] * lumi / metadata[sample]['nominal'][file_index]['nevts']\n    else:\n        xsec_weight = 1\n    return xsec_weight\n</code></pre> <p>Finally, we want to save our histogram in a format that can be interpreted by the <code>Combine</code> tool that we use for statistical analyses in CMS. There are many different formats that can be used, however we will use a simple <code>.csv</code> format for these exercises. </p> <p>I have created a simple conversion function for you to use in the file <code>hist2df.py</code>. </p> <p>You can convert a histogram created with the matplotlib <code>hist</code> function to a dataframe using this function as follows, </p> Python<pre><code>weights =  plt.hist(observable,bins=np.arange(50,575,25))\ndfs = histogramToDataframe(weights[0],\"signalregion\",\"ttbar\")\n</code></pre> <p>We also want to save our histograms for the case where we shifted the jet energy scales (jes) up and down - eg,  Python<pre><code>weightsUp =  plt.hist(observableUp,bins=np.arange(50,575,25))\ndfsUp = histogramToDataframe(weightsUp[0],\"signalregion\",\"ttbar\",sys=\"jesUp\")\n</code></pre></p> <p>Since we haven't included the event weights when making the histogram, we will multiply the column <code>sum_w</code> by this weight.  Text Only<pre><code>event_weight  = getEventWeight('ttbar',0) # the weight needed when using only the first file \ndfs['sum_w']  *= event_weight \ndfs['sum_ww'] *= event_weight*event_weight\n</code></pre></p> <p>Note that there is also a column in our dataframe called <code>sum_ww</code> that we have multiplied by the square of the event weight. This column is the variance of the bin contents due to the limited number of MC simulated events available. We'll discuss more about this column in later exercises. </p> <p>Finally, we can save the histogram in the format needed later for <code>Combine</code> using,  Text Only<pre><code>dfs = df.sort_values(by=['channel','process','systematic','bin'])\ndfs.to_csv('histograms.csv',index=False)\n</code></pre> The first line is just to avoid some python warnings later on. You'll now see a file called <code>histograms.csv</code> in your file browser. </p>"},{"location":"day1/#full-analysis","title":"Full analysis","text":"<p>In this exercise, we only ran our selection and created a histogram for one of the files for one of our simulated processes. We should run the same selection over all of the simulated files and for the data too!</p> <p>Question</p> <p>Write some code that will run over all of the simulated sample files and all of the data files too. Remember, </p> <ol> <li>Only look at the samples called <code>nominal</code> in the <code>.json</code> file.  </li> <li>Make a separate histogram for each of the different processes and for the data. Each simulated sample will have its own event weight that you'll need to calculate. </li> <li>Remember to include a histogram for the up and down variations of the jet energy scale for every sample except the data.  </li> <li>You can save all of the histograms to the same <code>.csv</code> file or you can save them in separate files.</li> </ol> <p>This may take some time so you should get some working code and then you may need to let the commands run for a couple of hours - maybe leave it running over dinner or even overnight.  Remember, if you don't access the full set of files, you should recalculate the integrated luminosity (you can assume that the new integrated luminosity scales with the number of events accessed) and modify the function to calculate the event weight to account for missing simulated events. </p> <p>Feel free to choose your own binning for your observable or you could even choose a different observable to the one we calculated in the example above! You might even try saving different histograms with different binnings/observables so that you can see what difference this makes for the statistical analysis later. </p> <p>Please try to write your own code before looking at the answer provided. </p> Show answer <p>I have uploaded a Jupyter notebook that will perform the full analysis on the samples called <code>FullAnalysis.ipynb</code>in the <code>ttbarAnalysis</code> folder. If you really get stuck, have a look at this notebook to see how to run our object and event selection, calculate the observable and save the histograms for all of the samples. </p> <p>The notebook will produce a plot similar to the one below,</p> <p></p>"},{"location":"day2/","title":"Exercise 2 - Maximum Likelihood Fits","text":"<p>From yesterday's exercise, we now have a set of histograms, from data and simulation, in our <code>.csv</code> file. Don't worry if you didn't manage to run over all of the samples from yesterday, you can use the pre-prepared file <code>ttbarAnalysis/exercise1solutions/signalregion_mbjj.csv</code>.</p> <p>In today's exercises, we're going to use the CMS statistics software tool <code>combine</code> to extract statistical results from the data (and simulation) that we processed yesterday. <code>combine</code> is a software package that is designed with a command line interface that uses simple <code>.txt</code> files as inputs. You can find out lots more about the tool at the online documentation pages here.</p> <p>We'll begin by starting the container that has <code>combine</code> compiled for us. If you didn't download the container already, go back to the Getting started pages before continuing. </p> <p>To do this, type the following into a terminal on your laptop (or by clicking the play button next to the <code>cms_combine</code> container in the Docker desktop application and using the terminal there).</p> Bash<pre><code>docker start -i cms_combine\n</code></pre> <p>We'll also need to checkout the GitHub area since this is the first time we're using this container.</p> Bash<pre><code>git clone https://github.com/nucleosynthesis/LHCDataStatisticsICISE2024.git\ncd LHCDataStatisticsICISE2024/ttbarAnalysis \n</code></pre> <p>Now that we're inside the container, start jupyter lab and enter the URL that gets printed to the screen in your preferred browser. </p> Bash<pre><code>jupyter lab --ip 0.0.0.0 --port 8889 --no-browser\n</code></pre>"},{"location":"day2/#datacards","title":"Datacards","text":"<p>Create a new Text file and give it a name (I called mine <code>signalregion_mbjj.txt</code>). This text file will be the input to our <code>combine</code> commands - we call it a Datacard. Copy the text below into your text file. </p> Text Only<pre><code>imax 1\njmax 4\nkmax 0\n# -----------------------------------------------------------------------------------------\nshapes data_obs signalregion signalregion_mbjj.csv signalregion:data:nominal,sum_w:sum_ww\nshapes *        signalregion signalregion_mbjj.csv signalregion:$PROCESS:nominal,sum_w:sum_ww \n# -----------------------------------------------------------------------------------------\nbin         signalregion\nobservation -1\n# -----------------------------------------------------------------------------------------\nbin         signalregion  signalregion        signalregion       signalregion   signalregion\nprocess     ttbar         single_atop_t_chan  single_top_t_chan  single_top_tW  wjets\nprocess     0             1                   2                  3              4\nrate        -1            -1                  -1                 -1             -1\n# -----------------------------------------------------------------------------------------\n</code></pre> <p>Let's go through these lines and see what they are doing. </p> <p>The first lines, specifying <code>imax</code>, <code>jmax</code> and <code>kmax</code> indicate the number of channels, backgrounds and systematic uncertainties, respectively. In our case, we only have one channel (the signal region), and we have four background processes. Right now, we don't have any systematic uncertainties included but later on, we'll add some - for now, we keep this as 0. </p> <p>The next lines (starting with <code>shapes</code>) say where <code>combine</code> will find the distributions (histograms) for the data and the different processes. The first line,  Text Only<pre><code>shapes data_obs signalregion signalregion_mbjj.csv signalregion:data:nominal,sum_w:sum_ww\n</code></pre> says that the shape (distribution or histogram if you like) for the observed data can be found in our <code>signalregion_mbjj.csv</code> file in the rows where <code>process==data</code> and <code>systematic==nomial</code>. The final parts of the line say that the column <code>sum_w</code> represents the number of events in each bin. For the data <code>sum_ww</code> isn't used but this part of the line is needed for the code to work. </p> <p>The next line is very similar  Text Only<pre><code>shapes *        signalregion signalregion_mbjj.csv signalregion:$PROCESS:nominal,sum_w:sum_ww \n</code></pre> This line is telling <code>combine</code> where to find the shapes for the signal and background processes. Here the <code>*</code> means that this applies for all of the processes. The keyword <code>$PROCESS</code> is expanded for each process that we define in the datacard. This is useful to avoid having to write the same datacard line over and over. </p> <p>Next, we have a line that indicates the total numbers of events for the observed data and the different processes in our signal region.  Text Only<pre><code>bin         signalregion\nobservation -1\n</code></pre> This tells <code>combine</code> that there is a channel called <code>signalregion</code>. The observation is usually a number but the <code>-1</code> there tells <code>combine</code> to go and calculate the total number of observed events by summing up the number of events in each bin - this saves us having to write the number each time.  The next lines are similar,  Text Only<pre><code>bin         signalregion  signalregion        signalregion       signalregion   signalregion\nprocess     ttbar         single_atop_t_chan  single_top_t_chan  single_top_tW  wjets\nprocess     0             1                   2                  3              4\nrate        -1            -1                  -1                 -1             -1\n</code></pre></p> <p>This tells <code>combine</code> which processes are expected to contribution to the signal region. The names given should match the names in our <code>.csv</code> file so that the <code>shapes</code> line above will find the right histograms. Again, we use <code>-1</code> for the rate so that <code>combine</code> does the calculating for us. Finally, we also give a process ID. Any process with a value \\leq will be considered a signal process, while any process with a value &gt;0 will be a background process. This is important later when we perform statistical tests. </p>"},{"location":"day2/#performing-a-first-fit","title":"Performing a first fit","text":"<p>Now that we have our Datacard written. We can perform some statistical calculations. We can perform a fit to the data, allowing the total rate of the <code>ttbar</code> process to vary. <code>combine</code> will automatically create a signal strength parameter - <code>r</code> which multiplies the rate of any signal process defined in the datacard. </p> <p>We will use the <code>FitDiagnostics</code> method to extract the best fit value for the parameter <code>r</code> given the data we observed. We do this by typing the following command in a terminal (you can open a terminal in  jupyter lab), </p> Bash<pre><code>combine signalregion_mbjj.txt -M FitDiagnostics\n</code></pre> <p>You should get something similar to the output below  Text Only<pre><code> --- FitDiagnostics ---\nBest fit r: 0.903186  -0.00366585/+0.00367476  (68% CL)\nDone in 0.01 min (cpu), 0.01 min (real)\n</code></pre> You will likely get some warnings about <code>PDF didn't factorize!</code> that you can safely ignore for now.</p> <p>So it looks like the fit would prefer a value of the tt cross-section around 10% smaller than the predicted value that we used when we processed the ttbar sample. You may have a different result if you are using your own <code>.csv</code> file. </p> <p>Note that you also will now have files called <code>higgsCombineTest.FitDiagnostics.mH120.root</code> and <code>fitDiagnosticsTest.root</code> that got created from the above command. These files save the result in various ROOT object formats. We will use these  files later on, but for now you can just ignore this file. </p> <p>Let's see if our data/simulation agreement has improved after the fit. We can do this by asking combine to create new templates after the fit. We run the command again, but this time, we will add options to save templates with uncertainties to the <code>fitDiagnosticsTest.root</code> file. </p> Text Only<pre><code>combine signalregion_mbjj.txt -M FitDiagnostics --saveShapes --saveWithUncert\n</code></pre> <p>If you are unfamiliar with ROOT, I have created a file called <code>root2py.py</code> with some handy functions to read and convert the objects into simple python arrays that can be used for plotting with matplotlib. Below is an example (written in pyROOT or just python) to make a plot of the distributions after the fit. You can copy either of these </p> pythonpyROOT Python<pre><code>from root2py import *\nimport ROOT \n\nfile   = ROOT.TFile.Open(\"fitDiagnosticsTest.root\")\nfolder = file.Get(\"shapes_fit_s/signalregion\")\n\n# function getHistogramCountsAndData returns dictionary with format: \n#   {\n#    \"data\"    : [[x],[y],[ey_l],[ey_h]]\n#    ,\"total\"  : [[bin_edges],[y],[ey]]\n#    ,\"samples\": ['name',[[bin_edges],[y],[ey]]]\n#   }\n\nresults = getHistogramCountsAndData(folder)\n\nimport matplotlib.pyplot as plt\n\nnsamples    = len(results['samples'])\nbin_centres = results['data'][0]\nbins_list   = [results['samples'][i][1][0][0:-1] for i in range(nsamples)]\nbin_boundaries = results['samples'][0][1][0]\nsamples_stack = [results['samples'][i][1][1] for i in range(nsamples)]\nlabels        = [results['samples'][i][0] for i in range(nsamples)]\ndata_errs     = [results['data'][2],results['data'][3]]\n\nplt.hist(bins_list,bins=bin_boundaries,weights=samples_stack,label=labels,stacked=True,density=False)\nplt.errorbar(bin_centres,results['data'][1]\n             ,yerr=data_errs\n             ,label='data'\n             ,marker='o'\n             ,markersize=4.0\n             ,color='black'\n             ,linestyle=\"none\")\n\n# calculate min and max from total errors on prediction for filled area\ntotalED = results['total'][1]-results['total'][2]\ntotalEU = results['total'][1]+results['total'][2]\n\nplt.fill_between(bin_centres,totalED,totalEU,color='gray',alpha=0.25,step='mid')\nplt.ylim(0,1.3e4)\nplt.legend()\n</code></pre> Python<pre><code>import ROOT\n\ncanvas = ROOT.TCanvas()\n\nfile   = ROOT.TFile.Open(\"fitDiagnosticsTest.root\")\nfolder = file.Get(\"shapes_fit_s/signalregion\")\n\ndata  = folder.Get(\"data\")\nprint(data.GetN())\ntotal = folder.Get(\"total\"); total.SetFillColor(ROOT.kGray)\n\nttbar = folder.Get(\"ttbar\") ; ttbar.SetFillColor(ROOT.kOrange) ; ttbar.SetFillStyle(1001)\nwjets = folder.Get(\"wjets\") ; wjets.SetFillColor(ROOT.kRed+1)\nsingle_top_tW = folder.Get(\"single_top_tW\") ; single_top_tW.SetFillColor(ROOT.kBlue)\nsingle_atop_t_chan = folder.Get(\"single_atop_t_chan\"); single_atop_t_chan.SetFillColor(ROOT.kBlue-3)\nsingle_top_t_chan  = folder.Get(\"single_top_t_chan\"); single_top_t_chan.SetFillColor(ROOT.kBlue-9)\n\n# create legend and stack and fill them\nlegend = ROOT.TLegend(0.6,0.6,0.89,0.89)\nlegend.AddEntry(data,\"data\",\"pe\")\n\nstk = ROOT.THStack(\"stack\",\";;Events\")\nstk.Add(single_top_t_chan); legend.AddEntry(single_top_t_chan,\"single_top_t_chan\",\"F\")\nstk.Add(single_atop_t_chan); legend.AddEntry(single_atop_t_chan,\"single_atop_t_chan\",\"F\")\nstk.Add(single_top_tW); legend.AddEntry(single_top_tW,\"single_top_tW\",\"F\")\nstk.Add(wjets); legend.AddEntry(wjets,\"wjets\",\"F\")\nstk.Add(ttbar); legend.AddEntry(ttbar,\"ttbar\",\"F\")\n\n#data.Draw(\"ApE\")\nstk.Draw(\"hist\")\ndata.Draw(\"pE\")\n#ttbar.Draw(\"histsame\")\ntotal.Draw(\"E2same\")\nlegend.Draw()\ncanvas.Draw()\n</code></pre> <p>Challenge</p> <p>Add a ratio panel below the main plot that shows data/total expected. This can be a good way to judge more easily by eye how well the data and predictions agree after the fit. </p>"},{"location":"day2/#adding-additional-rate-modifiers","title":"Adding additional rate modifiers","text":"<p>The agreement looks a little better than before but we could still do better. We should also allow our dominant background process (<code>wjets</code>) to float so that the fit can adjust the total rate. </p> <p>Add the following line to your Datacard,  Text Only<pre><code>wjets_norm rateParam signalregion wjets 1 [0,5]\n</code></pre></p> <p>This line tells combine to add a new parameter to the model that multiplies the rate of the specified process, in the specified channel. The new parameters will be <code>wjets_norm</code>, which multiplies the rate of the <code>wjets</code> process. We've chosen the default value of the parameter to be 1 and allowed its range to vary between 0 and 5. </p> <p>Question</p> <p>Run the <code>-M FitDiagnostics</code> method again and plot the new best fit shapes. Does the agreement improve compared to before? What are the values of the fitted rate multipliers (add the option <code>-v 3</code> to get combine to print this information).</p> Show answer <p>By running Text Only<pre><code>combine signalregion_mbjj.txt -M FitDiagnostics -v 3 --saveShapes --saveWithUncert\n</code></pre> we now find in the output  Text Only<pre><code>r          = 0.824465     +/-  0.0115119 (limited)\nwjets_norm = 2.45416      +/-  0.203045  (limited)\n</code></pre> Now we see that the fit would prefer to scale up the <code>wjets</code> process by a factor of ~2.45 compared to our original prediction. The best-fit value for the signal strength has also changed from what we saw before. </p> <p>If we look at a plot of the fit results we can see things are a little improved. The plot below is made using the pyROOT version of our plotting code. </p> <p></p> <p>Remember, your results might be different if you're using your own solution from the first exercise. </p> <p>You may have noticed that in addition to the best fit value, <code>combine</code> reports an uncertainty <code>+/-</code>. We will cover what we mean by uncertainty in the lectures, but this uncertainty is estimated by using the inverse of the Hessian matrix from the fit result. From this alone, we can see that by including an additional parameter, the uncertainty on our best fit for <code>r</code> has increased. Generally, we expect that adding freedom to the fit will increase the uncertainty of our results but improve the overall agreement in our data/simulation plots.  </p> <p>We can estimate this uncertainty more accurately by scanning the profiled log-likelihood and comparing this for the two cases; with and without the <code>wjets_norm</code> parameter. </p>"},{"location":"day2/#likelihood-scans","title":"Likelihood scans","text":"<p>Remember in the lectures, we define a quantity q(r) as the following, </p>  q(r) = -2\\ln \\frac{L(r,\\hat{\\nu}_{r})}{L(\\hat{r},\\hat{\\nu})}  <p>where the \\hat{\\cdot} notation means the maximum likelihood estimate (or best-fit) and the subscript and where \\nu represents our nuisance parameters. In this case, we only have one such nuisance parameter which is <code>wjets_norm</code>. The value \\hat{\\nu}_{r} is the value of \\nu for that maximises the likelihood when r is fixed to a specific value - sometimes we call this the conditional maximum likelihood. </p> <p><code>combine</code> can calculate this quantity as a function of our parameter r using the <code>-M MultiDimFit</code> method and the <code>grid</code> algo with the command, </p> Bash<pre><code>combine signalregion_mbjj.txt -M MultiDimFit --algo grid --points 50 --setParameterRanges r=0.7,1\n</code></pre> <p>The option <code>--points</code> tells combine how many evenly spaced points at which to evaluate the function q(r) and the last option specifies the range that those points should be in. After running this command, there will be a new ROOT file called <code>higgsCombineTest.MultiDimFit.mH120.root</code> in your file browser. </p> <p>Warning</p> <p>If you run the <code>combine</code> command twice, the output file will be overwritten. By adding the option <code>-n name</code>, you can avoid this as the word <code>Test</code> will be modified to <code>name</code> that you specify. Use this to keep track of different results.  </p> <p>This file contains a <code>TTree</code> that stores the values of r and 0.5\\times q(r) in its branches. In the python file <code>root2py.py</code> I have included a function to convert these into python arrays for you. You can plot the value of q(r) using one of the blocks of code below, depending on if you prefer pyROOT or straight python. </p> pythonpyROOT Python<pre><code>from root2py import *\nimport ROOT\nfscan = ROOT.TFile(\"higgsCombineTest.MultiDimFit.mH120.root\")\ntree  = fscan.Get(\"limit\")\nx,y = get2DeltaNLLScan(tree)\nplt.plot(x,y) \n</code></pre> Python<pre><code>import ROOT\nfscan = ROOT.TFile(\"higgsCombineTest.MultiDimFit.mH120.root\")\ntree  = fscan.Get(\"limit\")\ncanvas = ROOT.TCanvas()\ntree.Draw(\"2*deltaNLL:r\",\"\",\"L\")\ncanvas.Draw()\n</code></pre> <p>Question</p> <p>Make a plot of q(r) for the case where the rate of <code>wjets</code> is allowed to float and where it is fixed to the original value, on the same axis. Which one is wider? What does that tell us?</p> Show answer <p>You should find something like the plot below by using the code provided above and modifying to plot two scans on top of one another. </p> <p></p> <p>The scan where the rate of <code>wjets</code> is allowed to float is much wider than the original scan. </p> <p>Challenge</p> <p>To estimate the uncertainty, we can define confidence intervals, according to Wilks' theorem, as the region for which q(r)&lt; x_{\\alpha} where \\alpha is our test size. For 1-\\alpha=0.683, i.e a 68.3% confidence interval, x_{\\alpha}=1. Find the 68.3% intervals for each of the scans using this method. </p>"},{"location":"day2/#goodness-of-fit","title":"Goodness-of-fit","text":"<p>Instead of comparing our data/simulation plots by eye, we can use a measure of the Goodness-of-fit. In lectures we learned about hypothesis testing. <code>combine</code> has several routines for calculating a goodness-of-fit, but we will just use the so called <code>saturated</code> test for an idea of the fit quality. </p> <p>The test-statistic for a template model is, </p> <p>$$     t = -2\\ln\\frac{L(\\hat{r},\\hat{\\theta})}{L_{s}} $$ where L_{s} is the saturated likelihood. It is defined as the likelihood valye where the expected value in each bin is exactly the same as the observed value - i.e the very largest possible likelihood value given the data observed. </p> <p>We can calculate this value with <code>combine</code> using the following,  Bash<pre><code>combine signalregion_mbjj.txt -M GoodnessOfFit --algo=saturated\n</code></pre></p> <p>A result in the literature (Wilks' theorem) tells us that for large statistics samples (in the asymptotic limit), this test statistic will be distributed according to a \\chi^{2} distribution with n degrees of freedom where n is the number of bins in the distribution minus the number of parameters in the fit - in our case this is n=20-2=18. This means, we can convert the number calculated by combine to a p-value using, </p>  p = \\int_{t}^{+\\infty} \\chi^{2}(n)dt   <p>or in code form using the function below  Python<pre><code>import ROOT \nROOT.TMath.Prob(t,18)\n</code></pre></p> <p>Question</p> <p>Calculate the p-value for both cases (with and without the <code>wjets_norm</code> parameter). Which one is better? </p> Show answer <p>You should find a very very small value in both cases so both fits are pretty awful. </p> <p>Challenge</p> <p>In the <code>combine</code> online documentation, on Goodness of fit tests, you can find instructions for calculating p-values using toys rather than relying on asymptotic methods. Calculate the p-values for the two cases using the saturated algorithm.</p>"},{"location":"day2/#control-regions","title":"Control regions","text":"<p>In the previous examples we saw that we can trade between sensitivity in our measurement (the uncertainty in <code>r</code>) and quality of the fit result. In an ideal world, we would have a good overall quality of the fit and a reasonably small uncertainty in our results. </p> <p>We saw that the fit improved when allowing the rate of <code>wjets</code> to float in the fit, but at the cost of increased uncertainty in <code>r</code>. We can improve this by constraining the parameter <code>wjets_norm</code> using a control region. </p> <p>A control region is another region in the data that is dominated by a particular background process. By including this region in the fit, we will be able to use that additional data to constrain the <code>wjets_norm</code> parameter. </p> <p>Let's go back to the <code>cms_python</code> container and modify our analysis notebook to create a control region that will be dominated by the <code>wjets</code> process. If you didn't manage to get a working analysis yesterday, you can use the notebook provided <code>ttbarAnalysis/exercise1solutions/FullAnalysis.ipynb</code>. </p> <p>We need to change how we are filtering our events, we want to modify the selection so that we choose events with no b-tagged jets but everything else is the same, i.e change,   </p> <p>Text Only<pre><code>(ak.sum(selected_jets.btag &gt;= 0.8, axis=1) &gt;= 2)\n</code></pre> to  Text Only<pre><code>(ak.sum(selected_jets.btag &gt;= 0.8, axis=1) &lt; 1)\n</code></pre></p> <p>Our observable will be different now since we no longer want to pick the trijet with a b-quark. Instead we just use the trijet with the largest p_{T}, i.e we change the observable calcualtion to  Text Only<pre><code>trijet = ak.combinations(selected_events_CR, 3, fields=[\"j1\", \"j2\", \"j3\"])\nj1,j2,j3 = ak.unzip(trijet)\ntrijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3\ntrijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\nobservable = ak.flatten(trijet_mass).to_numpy()\n</code></pre> for the control region. We will use this control region in combination with our signal region in tomorrow's exercise. We will refer to our control region as 4j0b since we explicitly asked that none of the jets are tagged as coming from a b-quark. Instead our signal region is 4j2b since in addition to requiring 4 jets overall, we asked that at least two of them are tagged as originating from a b-quark. </p> <p>Question</p> <p>Write a new notebook to process the events but this time applying the control region selection.  If you prefer, you can instead modify your original notebook so that it can process the events for the signal region and control region at the same time! Remember,     - Also modify the name of the output file to something like <code>controlregion_mbjj.csv</code> so as not to overwrite our signal region.     - You should name the channel for the control region something like \"<code>controlregion</code>\" as we'll use this name for our datacards later - remember this when using the <code>histogramToDataframe</code> function.     - We also want to include the histograms for the jes up and down variations just like we did in the control region. </p> Show answer <p>I have uploaded a Jupyter notebook that will perform the  analysis on the samples and produce both the signal region and control region in a single output <code>.csv</code> called <code>allregions.csv</code>. The notebook os called <code>FullAnalysisWithControlRegion.ipynb</code>in the <code>ttbarAnalysis</code> folder. If you really get stuck, have a look at this notebook to see how to run our object and event selection, calculate the observable and save the histograms for all of the samples this time with the control region. </p> <p>The distributions in the control region should look something like the plot below </p> <p></p>"},{"location":"day3/","title":"Exercise 3 - Control Regions and Systematic Uncertainties","text":"<p>Launch the <code>cms_combine</code> container by typing the following into a terminal on your laptop (or by clicking the play button next to the cms_combine container in the Docker desktop application and using the terminal there). Bash<pre><code>docker start -i cms_combine\n</code></pre></p> <p>In today's exercise, we are going to use our 4j0b control region that we populated at the end of exercise 2 to constrain our <code>wjets</code> process in our 4j2b signal region. Don't worry if you didn't manage to process the samples to create the histograms for the 4j0b region. I have put a <code>.csv</code> file  <code>exercise2solutions/allregions_mbjj.csv</code> that has both the signal region and control region histograms for you. You'll also find the datacard for the signal region in the same folder: <code>signalregion_mbjj.txt</code>. </p>"},{"location":"day3/#control-region-datacard","title":"Control region datacard","text":"<p>First, we need to create a new datacard for our 4j0b control region. This will look very similar to our signal region datacard except that we need to change the name of the channel and point to the right histograms in the file. Copy the text below into a new text file, i've called mine <code>controlregion_mjjj.txt</code></p> Text Only<pre><code>imax 1\njmax 4\nkmax 0\n# -----------------------------------------------------------------------------------------\nshapes data_obs controlregion allregions_mbjj.csv controlregion:data:nominal,sum_w:sum_ww\nshapes *        controlregion allregions_mbjj.csv controlregion:$PROCESS:nominal,sum_w:sum_ww \n# -----------------------------------------------------------------------------------------\nbin         controlregion\nobservation -1\n# -----------------------------------------------------------------------------------------\nbin         controlregion controlregion       controlregion      controlregion  controlregion\nprocess     ttbar         single_atop_t_chan  single_top_t_chan  single_top_tW  wjets\nprocess     0             1                   2                  3              4\nrate        -1            -1                  -1                 -1             -1\n# -----------------------------------------------------------------------------------------\nwjets_norm rateParam controlregion wjets 1 [0,5]\n</code></pre> <p>Notice how the <code>shapes</code> lines are now pointing to the <code>controlregion</code> histograms in the <code>allregions_mbjj.csv</code> file. The names of the channel everywhere has also changed to <code>controlregion</code>. This tells combine that this data are independent from the data in our signal region. Finally, the last line of the datacard,  Text Only<pre><code>wjets_norm rateParam controlregion wjets 1 [0,5]\n</code></pre> tells combine to create a parameter called <code>wjets_norm</code> that modifies the rate of the <code>wjets</code> process in the <code>controlregion</code>. By naming this parameter the same as the one in our signal region datacard, this parameter will simultaneously scale the <code>wjets</code> process in both regions!</p> <p>Now we need to create a single datacard that contains the information for both the signal region and the control region. We do not need to do this by hand since the package comes with a tool that does this for us. To create combined datacard, we can run the following, </p> Bash<pre><code>combineCards.py signalregion=signalregion_mbjj.txt controlregion=controlregion_mjjj.txt &gt; combined.txt \n</code></pre> <p>Let's look at the resulting datacard (below) Text Only<pre><code>Combination of signalregion=signalregion_mbjj.txt  controlregion=controlregion_mjjj.txt\nimax 2 number of bins\njmax 4 number of processes minus 1\nkmax 0 number of nuisance parameters\n----------------------------------------------------------------------------------------------------------------------------------\nshapes *              controlregion  allregions_mbjj.csv controlregion:$PROCESS:nominal,sum_w:sum_ww\nshapes data_obs       controlregion  allregions_mbjj.csv controlregion:data:nominal,sum_w:sum_ww\nshapes *              signalregion   allregions_mbjj.csv signalregion:$PROCESS:nominal,sum_w:sum_ww\nshapes data_obs       signalregion   allregions_mbjj.csv signalregion:data:nominal,sum_w:sum_ww\n----------------------------------------------------------------------------------------------------------------------------------\nbin          signalregion   controlregion\nobservation  -1             -1           \n----------------------------------------------------------------------------------------------------------------------------------\nbin          signalregion        signalregion        signalregion        signalregion        signalregion        controlregion       controlregion       controlregion       controlregion       controlregion     \nprocess      ttbar               single_atop_t_chan  single_top_t_chan   single_top_tW       wjets               ttbar               single_atop_t_chan  single_top_t_chan   single_top_tW       wjets             \nprocess      0                   1                   2                   3                   4                   0                   1                   2                   3                   4                 \nrate         -1                  -1                  -1                  -1                  -1                  -1                  -1                  -1                  -1                  -1                \n----------------------------------------------------------------------------------------------------------------------------------\nwjets_norm    rateParam signalregion wjets 1 [0,5]\nwjets_norm    rateParam controlregion wjets 1 [0,5]\n</code></pre></p> <p>We now see that all of the information has been combined into a single datacard! at the top of the datacard, we see that the datacard is a result of the combination of our two original datacards, and the numbers <code>imax</code>, <code>jmax</code> and <code>kmax</code> have automatically been updated for us. </p> <p>Warning</p> <p>In the combined datacard, both the signal region and control region histograms are taken from the same file <code>allregions_mbjj.csv</code>, this is because I have changed the <code>shape</code> lines in my signal region datacard to read  Text Only<pre><code>shapes data_obs signalregion allregions_mbjj.csv signalregion:data:nominal,sum_w:sum_ww\nshapes *        signalregion allregions_mbjj.csv signalregion:$PROCESS:nominal,sum_w:sum_ww \n</code></pre> You do not have to do this if you created the signal region and control region histograms in separate <code>.csv</code> files. </p> <p>This datacard can now be used as the input to out <code>combine</code> commands to perform fits and calculate uncertainties as we did when we only had a single datacard. </p> <p>Question</p> <p>Run the fit diagnostics method on the combined datacard to calculate the best fit value for the signal strength parameter <code>r</code>. How does this compare to the two results we obtained with a single datacard? Make a plot of the post-fit distributions and the data in the two regions from this new fit. Remember to add the option <code>-n name</code> to create a different name for the output file to avoid writing over your original outputs from combine.  </p> Show answer <p>We can run the same commands as before, but this time we provide the combined datacard as the input. For the best fit, Bash<pre><code>combine combined.txt -M FitDiagnostics --saveShapes --saveWithUncert -n Combined\n</code></pre> with the result as,  Text Only<pre><code>--- FitDiagnostics ---\nBest fit r: 0.917371  -0.00376498/+0.00377766  (68% CL)\n</code></pre> The result and uncertainty is much closer to the version of the datacard without the freely floating <code>wjets_norm</code> parameter. This means we've successfully managed to recover the constraint on the <code>wjets_norm</code> parameter by using the data in the 4j0b control region! You can use the same code from Exercise 2 to plot each of the regions from the result file <code>fitDiagnosticsCombined.root</code></p>"},{"location":"day3/#systematic-uncertainties","title":"Systematic Uncertainties","text":"<p>In our fits so far, we have assumed that the distributions and rates determined from the simulated events represent perfectly what we would expect to see in data. Of course, reality is never quite that easy and every step of any real data analysis will involve some assumption about how well we can model a particular effect. Each of these assumptions comes with some uncertainty and these uncertainties impact the predicted yields and distributions - we call them \"systematic uncertainties\". These could range from uncertainties in the theoretical cross-sections used to normalise the samples, uncertainties in the efficiency of the trigger or event selection, energy scale and other calibrations used to reconstruct the particles or even the uncertainty on the integrated luminosity of our data set!</p> <p>The vast majority of the time spent doing a real LHC data analysis is carefully understanding which uncertainties effect any particular measurement and how large they are. Often, a lot of work is put into reducing these uncertainties (or rather their effect on the measurement) as much as possible to get the best measurements from the data. We don't have time in these exercises to properly calculate the effects of all the different systematic uncertainties that we should consider, but we will take a look at a few simple ones and include them in our analysis. </p> <p>In likelihood based methods (mostly what we use at the LHC), systematic uncertainties are included via nuisance parameters and auxiliary observables that can have different distributions. </p>"},{"location":"day3/#rate-uncertainties","title":"Rate uncertainties","text":"<p>The simplest form of systematic uncertainties are those that affect the overall predicted rate of a given process. We typically model rate uncertainties using the log-normal distribution. In <code>combine</code> this is implemented by multiplying the rate of a given process by a multiplicative factor, </p>  f(\\nu) = \\kappa^{\\nu}  <p>where \\nu is the associated nuisance parameter and \\kappa is the size of the effect of the uncertainty on a particular process. In <code>combine</code> an additional gaussian probability term in the likelihood gets included. This means that since the likelihood estimator \\hat{\\nu} is normal distributed, f(\\hat{\\nu}) will be log-normal distributed (hence the name). As we saw in lectures, the practical reason why we use log-normals is that f can never be negative, which makes sense for a rate. </p> <p>Add the following lines to your signal region datacard, </p> Text Only<pre><code>lumi  lnN   1.023        1.023                1.023              1.023          - \nideff lnN   1.01         1.01                 1.01               1.01           - \nbtag  lnN   1.03         1.03                 1.03               1.03           1.03 \n</code></pre> <p>Each of these lines tells <code>combine</code> that we have a new rate systematic uncertainty. Lets take a look at the first one of these <code>lumi</code>. The first parts of the line give the nuisance parameter parameter its name - <code>lumi</code> and its type <code>lnN</code> means its log-normal. The numbers represent the value of \\kappa for each process in our signal region. An uncertainty in the integrated luminosity measurement will scale all of the simulated processes up/down by the same amount - in our 2015 data, the uncertainty in the luminosity measurements was 2.3% so \\kappa is 1.023. </p> <p>If the parameter <code>lumi</code> is set to +1, the value of f will be 1.023 so all of the processes will increase by a factor 1.023. Similarly, if the parameter <code>lumi</code> is set to -1, the value of f will be 1.023 so all of the processes will change by 1/1.023 (decrease). </p> <p>Our <code>wjets</code> normalisation is determined by the fit - i.e from the data in the control region - so the luminosity systematic uncertainty should have no effect. In <code>combine</code> this is indicated by putting a \"<code>-</code>\" instead of a value. The next line <code>ideff</code> is exactly the same, except it is the systematic uncertainty for the muon/electron identification efficiency measurements and is 1%. </p> <p>The third line is for the b-tagging efficiency, and the uncertainty has an effect of 3% in the signal region. In the control region however, since we are rejecting b-jets, the effect of the uncertainty should be opposite (anti-correlated) to the signal region. In <code>combine</code> we achieve this by setting \\kappa &lt; 1.</p> <p>Question</p> <p>If the b-jet tagging efficiency increases the rate in the signal by 3%, what should the values of kappa be in the control region? Remember the effect should be opposite to the signal region and that since there is about 3 times as many events in the signal region as the control region, increasing the number of events that pass the b-tag (in our 4j2b region) by 3% will not result in a reduction of 3% events that fail the b-tag (in the 4j0b region). Add the systematic uncertainty lines for the <code>lumi</code>, <code>ideff</code> and <code>btag</code> to your control region datacard. </p> Show answer <p>The number should be \\kappa=0.99 in the control region. This is because adding 3% of the yield in the signal region to the control region would increase the control region yield by 1% since there's already 3 times as much data there. The rate uncertainty lines in the datacard for the control region should look like,  Text Only<pre><code>lumi  lnN   0.023        0.023                0.023              0.023          - \nideff lnN   1.01         1.01                 1.01               1.01           - \nbtag  lnN   0.99         0.99                 0.99               0.99           0.99 \n</code></pre></p> <p>Notice that for the <code>btag</code> uncertainty, there should be an entry for the <code>wjets</code>. This is because this  uncertainty has a different effect in the signal and control regions. </p>"},{"location":"day3/#shape-uncertainties","title":"Shape uncertainties","text":"<p>Some uncertainties in will affect both the overall rate of a particular process and its shape. For these uncertainties, we need to provide alternative histograms that represent the shape of a process when we vary some aspect of our model up and down. Remember, when we created our <code>.csv</code> files, we created histograms where we shifted the jet energy scale up and down by its uncertainty, resulting from uncertainties in the jet calibrations. Let's take a look at these distributions for the <code>ttbar</code> process. You can use the following code in a notebook to plot these distributions, </p> Text Only<pre><code>import matplotlib.pyplot as plt \nimport pandas as pd \n\ndf = pd.read_csv(\"allregions_mbjj.csv\")\n\ndf = df[ (df.channel=='controlregion') &amp; (df.process=='ttbar') ]\nbins = df[df.systematic=='nominal']['bin']\nprint(len(bins))\nplt.hist(bins,bins=bins\n         ,weights=df[df.systematic=='nominal']['sum_w']\n         ,histtype='step'\n         ,color='blue'\n         ,label='nominal')\nplt.hist(bins,bins=bins\n         ,weights=df[df.systematic=='jesUp']['sum_w']\n         ,histtype='step'\n         ,color='green'\n         ,label='jesUp')\nplt.hist(bins,bins=bins\n         ,weights=df[df.systematic=='jesDown']['sum_w']\n         ,histtype='step'\n         ,color='red'\n         ,label='jesDown')\nplt.legend()\nplt.xlabel(\"Bin number\")\n</code></pre> <p>Which will produce a figure similar to the one below, </p> <p></p> <p>We can see that when the jet energy scale is increased, the observable distribution shifts to the right, while when it is decreased, the observable distribution shifts to the left! This uncertainty will change our prediction of what the <code>ttbar</code> shape looks like (and in fact the other processes too). </p> <p>We need to tell <code>combine</code> about these histograms to create shape systematic uncertainties. First, we need to extend the <code>shapes</code> lines in the datacards to point to these histograms. We do this by modifying the <code>shapes</code> line of our <code>signalregion_mbjj.txt</code> datacard to read  Text Only<pre><code>shapes *        signalregion allregions_mbjj.csv signalregion:$PROCESS:nominal,sum_w:sum_ww signalregion:$PROCESS:$SYSTEMATIC,sum_w:sum_ww \n</code></pre> Notice how there is a new part at the end of the line with <code>$SYSTEMATIC</code>. This tells <code>combine</code> that each shape systematic uncertainty templates can be found in the same <code>.csv</code> file but will correspond to the column <code>systematic==SYSTEMATICUp</code> and <code>systematic==SYSTEMATICDown</code>. The value of <code>SYSTEMATIC</code> is determined from each of the shape uncertainty lines we add at the end of the datacard. We have only one which we called <code>jes</code> so we just need to add the following line to our signal region datacard, </p> Text Only<pre><code>jes   shape     1          1           1            1            1\n</code></pre> <p>This line tells <code>combine</code> to create a nuisance parameter for our shape uncertainty with the name <code>jes</code>. Each number indicates how big the variation that templates corresponds to is, for each process. The value <code>1</code> means that each alternate histogram corresponds to a 1-sigma variation. If we put a <code>-</code> it would mean there is no effect for this process. </p> <p>We also need to add the same to our control region datacard since the same source of uncertainty (and hence the same parameter) will simultaneously modify the shapes of our processes in the 4j0b and 4j2b regions. </p> <p>Question</p> <p>Extend the <code>shapes</code> lines in the control region datacard and add the systematic uncertainty line in the control region datacard too. </p> Show answer <p>First, we need to modify the <code>shapes</code> lines in the control region datcard as follows,  Text Only<pre><code>shapes *  controlregion allregions_mbjj.csv controlregion:$PROCESS:nominal,sum_w:sum_ww controlregion:$PROCESS:$SYSTEMATIC,sum_w:sum_ww \n</code></pre> similarly to what was done in the signal region. We also need to include the line that tells <code>combine</code> to create a shape uncertainty for the <code>jes</code> parameter at the end of the datacard,  Text Only<pre><code>jes   shape   1         1          1          1           1\n</code></pre></p> <p><code>combine</code> will create a new parameter, called <code>jes</code> and interpolate each bin of the histogram so that the histogram shape becomes a function of <code>jes</code>. The interpolating function is designed to be differentiable and so that we retrieve the nominal shape when <code>jes=0</code>, the <code>jesUp</code> shape when <code>jes=1</code> and the <code>jesDown</code> shape when <code>jes=-1</code>. </p> <p>First, let's build the statistical model for the signal region datacard, </p> Bash<pre><code>text2workspace.py signalregion_mbjj.txt -o signalregion_mbjj.root\n</code></pre> <p>Using the code below, we can plot the distribution of the <code>ttbar</code> process as we vary the <code>jes</code> parameter value, </p> pythonpyROOT Python<pre><code>from root2py import *\nimport ROOT\nimport numpy as np\n\nfile   = ROOT.TFile.Open(\"signalregion_mbjj.root \")\nworkspace = file.Get(\"w\")\n\nhists = []\njes_vals = np.arange(-2,2,0.25)\nfor jes_val in jes_vals:\n  workspace.var(\"jes\").setVal(jes_val)\n  hist = workspace.pdf(\"shapeSig_signalregion_ttbar_morph\").createHistogram(\"CMS_th1x\")\n  hist.SetName(\"hist_%g\"%jes_val)\n  hists.append(hist)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cm\n\nnormalize = mcolors.Normalize(vmin=jes_vals.min(), vmax=jes_vals.max())\ncolormap = cm.Reds\n\nfor i,jes_val in enumerate(jes_vals):\n    h = hists[i]\n    rx,ry,ey = readHist(h)\n    plt.hist(rx[0:-1],bins=rx,weights=ry\n            ,histtype='step',label='jes = %g'%jes_val\n            ,color=colormap(normalize(jes_val)))\n\nplt.xlabel(\"Bin number\")\nplt.legend()\nplt.tight_layout()\n</code></pre> Python<pre><code>import ROOT\nimport numpy as np\n\nfile   = ROOT.TFile.Open(\"signalregion_mbjj.root \")\nworkspace = file.Get(\"w\")\n\nhists = []\njes_vals = np.arange(-2,2,0.25)\n\nfor jes_val in jes_vals:\n  workspace.var(\"jes\").setVal(jes_val)\n  hist = workspace.pdf(\"shapeSig_signalregion_ttbar_morph\").createHistogram(\"CMS_th1x\")\n  hist.SetName(\"hist_%g\"%jes_val)\n  hists.append(hist)\n\nROOT.gStyle.SetOptStat(0)\nc = ROOT.TCanvas()\nleg = ROOT.TLegend(0.6,0.3,0.89,0.89)\nleg.SetBorderSize(0)\n\nfor i,jes_val in enumerate(jes_vals):\n    hists[i].SetTitle(\"\")\n    hists[i].GetXaxis().SetTitle(\"Bin Number\")\n    hists[i].GetYaxis().SetTitle(\"\")\n    hists[i].SetLineWidth(2)\n    if i==0: hists[i].Draw(\"PLC\")\n    else: hists[i].Draw(\"histsame PLC\")\n    leg.AddEntry(hists[i],\"jes = %g\"%jes_val,\"L\")\n    color+=1\n\nleg.Draw()\nc.Draw()\n</code></pre> <p>This will give a figure similar to the following, </p> <p></p>"},{"location":"day3/#monte-carlo-mc-statistics-uncertainties","title":"Monte-carlo (MC) statistics uncertainties.","text":"<p>Finally, we also need to account for the fact that our simulated samples (Monte-carlo or MC) have a limited number of events available to create the histograms. This leads to a statistical uncertainty on the histograms that are used in the fits. <code>combine</code> can take care of these for us by using the <code>sum_ww</code> column that we included in our dataframes. We just need to add the following line to the datacard,  Text Only<pre><code>signalregion autoMCStats 0\n</code></pre> The first part indicates which control region to consider and the <code>0</code> at the end is used to determine how the calculation is done (this is very technical ut if you want to find out about this you can read about it here). </p> <p>Question</p> <p>Add the relevant line to the control region datacard to account for the MC statistical uncertainties. </p> Show answer <p>We just need to add the following line to the <code>controlregion_mjjj.txt</code> datacard,  Text Only<pre><code>controlregion autoMCStats 0\n</code></pre></p> <p>Finally, we should make sure the <code>kmax</code> line in our datacards is adjusted to reflect the total number of systematic uncertainties. We do not count the <code>autoMCStats</code> line so we shoud set <code>kmax</code> to 4 in each datacard. </p>"},{"location":"day3/#putting-it-all-together","title":"Putting it all together","text":"<p>Now we have everything in place for the control region and systematic uncertainties, we should re-run the fit and extract the uncertainty on our measured signal rate <code>r</code>. </p> <p>Question</p> <ol> <li> <p>Run the <code>FitDiagnostics</code> method on the combination of the signal and control region and create a plot of the best fit for both regions.</p> </li> <li> <p>Determine the best fit parameter values. </p> </li> <li> <p>Calculate the \\chi^{2} from the best fit model. </p> </li> <li> <p>Perform a profiled likelihood scan of the signal strength <code>r</code> and plot it. </p> </li> </ol> <p>For 2. you can use the code below to print out the fit results. </p> pythonpyROOT Python<pre><code>from root2py import *\nimport ROOT\nfile   = ROOT.TFile.Open(\"fitDiagnosticsCombined.root\")\nfit_res = convertFitResult(file.Get(\"fit_s\"))\nprint(fit_res)\n</code></pre> Python<pre><code>import ROOT\nfile   = ROOT.TFile.Open(\"fitDiagnosticsCombined.root\")\nfit_res = file.Get(\"fit_s\")\nfit_res.Print()\n</code></pre> <p>Warning</p> <p>You may find that you get some errors in these steps related to the fit result. I advise you to add the <code>--cminDefaultMinimizerStrategy 0</code> to your <code>combine</code> commands. </p> Solution <ol> <li> <p>First we should create a new combined card with,  Bash<pre><code>combineCards.py signalregion=signalregion_mbjj.txt controlregion=controlregion_mjjj.txt &gt; combined.txt\n</code></pre> Next, we run the fit,  Bash<pre><code>combine combined.txt -M FitDiagnostics --skipB -n Combined --cminDefaultMinimizerStrategy 0 --saveShapes --saveWithUncert \n</code></pre> Using the output file we can find the results of the fit and also make plots of the. It should look something like the figure below,  </p> </li> <li> <p>Using the code provided, we get the following output (you should see something similar),  Text Only<pre><code>  Floating Parameter    FinalValue +/-  Error   \n  --------------------  --------------------------\n                  btag    2.1634e+00 +/-  8.54e-01\n                 ideff    4.6681e-01 +/-  1.02e+00\n                   jes    7.7198e-01 +/-  6.09e-02\n                  lumi   -3.7650e-01 +/-  1.01e-02\n  prop_bincontrolregion_bin0    7.9891e-01 +/-  8.83e-01\n  prop_bincontrolregion_bin1    2.1593e+00 +/-  9.00e-01\n  prop_bincontrolregion_bin10   -1.3274e-01 +/-  9.10e-01\n  prop_bincontrolregion_bin11    5.6041e-01 +/-  9.08e-01\n  prop_bincontrolregion_bin12   -1.1857e+00 +/-  9.12e-01\n  prop_bincontrolregion_bin13   -4.7468e-01 +/-  9.10e-01\n  prop_bincontrolregion_bin14   -2.6136e-01 +/-  9.08e-01\n  prop_bincontrolregion_bin15   -7.2318e-01 +/-  9.09e-01\n  prop_bincontrolregion_bin16   -4.2696e-02 +/-  9.06e-01\n  prop_bincontrolregion_bin17   -1.1439e+00 +/-  9.09e-01\n  prop_bincontrolregion_bin18   -4.3565e-01 +/-  9.08e-01\n  prop_bincontrolregion_bin19   -9.3066e-02 +/-  9.03e-01\n  prop_bincontrolregion_bin2    2.2050e+00 +/-  9.14e-01\n  prop_bincontrolregion_bin3    2.1998e+00 +/-  9.19e-01\n  prop_bincontrolregion_bin4    1.1193e+00 +/-  9.31e-01\n  prop_bincontrolregion_bin5   -1.2407e+00 +/-  9.34e-01\n  prop_bincontrolregion_bin6   -8.1615e-01 +/-  9.26e-01\n  prop_bincontrolregion_bin7   -7.9343e-01 +/-  9.15e-01\n  prop_bincontrolregion_bin8    1.1287e-01 +/-  9.13e-01\n  prop_bincontrolregion_bin9   -1.7256e-01 +/-  9.11e-01\n  prop_binsignalregion_bin0   -4.3051e-01 +/-  9.76e-01\n  prop_binsignalregion_bin1   -8.8268e-01 +/-  9.84e-01\n  prop_binsignalregion_bin10    6.6384e-01 +/-  9.85e-01\n  prop_binsignalregion_bin11    5.6890e-01 +/-  9.84e-01\n  prop_binsignalregion_bin12    5.6803e-01 +/-  9.83e-01\n  prop_binsignalregion_bin13    5.8144e-01 +/-  9.82e-01\n  prop_binsignalregion_bin14    2.1825e-01 +/-  9.82e-01\n  prop_binsignalregion_bin15    5.5108e-01 +/-  9.82e-01\n  prop_binsignalregion_bin16    4.5671e-01 +/-  9.81e-01\n  prop_binsignalregion_bin17    5.1578e-01 +/-  9.79e-01\n  prop_binsignalregion_bin18    6.7096e-01 +/-  9.79e-01\n  prop_binsignalregion_bin19    2.1684e-01 +/-  9.81e-01\n  prop_binsignalregion_bin2   -1.0313e+00 +/-  9.86e-01\n  prop_binsignalregion_bin3   -6.6500e-01 +/-  9.87e-01\n  prop_binsignalregion_bin4   -9.7874e-01 +/-  9.89e-01\n  prop_binsignalregion_bin5   -5.2094e-01 +/-  9.88e-01\n  prop_binsignalregion_bin6   -2.9065e-01 +/-  9.88e-01\n  prop_binsignalregion_bin7    1.5923e-01 +/-  9.87e-01\n  prop_binsignalregion_bin8    5.5739e-01 +/-  9.87e-01\n  prop_binsignalregion_bin9    4.2686e-01 +/-  9.86e-01\n                     r    8.7335e-01 +/-  2.67e-02\n            wjets_norm    5.0207e-01 +/-  1.47e-02\n</code></pre></p> </li> <li> <p>The \\chi^{2} is obtained first running the <code>GoodnessOfFit</code> method,  Bash<pre><code>combine combined.txt -M GoodnessOfFit --algo=saturated\n</code></pre> which gives a value of 335.387. We can convert it to a p-value again. </p> </li> <li> <p>We can run a scan of the profiled likelihood using <code>combine</code> with,  Bash<pre><code>combine combined.txt -M MultiDimFit --algo grid --points 50 --setParameterRanges r=0.7,1 -n Combined\n</code></pre> and plot using similar code to before. The output should look like the following,  </p> </li> </ol> <p>All of the code to produce these results can be found under <code>exercise3solutions/Answers.ipynb</code></p> <p>Challenge</p> <p>Calculate the uncertainty on <code>r</code> only considering statistical uncertainties and when also including systematic ones. This will involve freezing some of the parameters of the model to their maximum likelihood estimates - you can refer to information here on how this can be done. </p>"},{"location":"day4/","title":"Exercise 4","text":""},{"location":"day4/#searching-for-a-new-signal","title":"Searching for a new signal","text":"<p>We are now going to change our analysis from a measurement of a cross-section to a search for a new particle.  In our final exercise (Exercise 4), we are going to </p>"},{"location":"day4/#information-for-signal-search","title":"Information for signal search","text":"<p>events = NanoEventsFactory.from_root(     \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/RunIIFall15MiniAODv2_TprimeTprime_M-1200_TuneCUETP8M1_13TeV-madgraph-pythia8_flat/5A744C3D-EA4C-4C35-9738-BF878E063562_flat.root\",     , schemaclass=AGCSchema, treepath='events').events()</p> <p>filter_eff = 1  xsex = 0.0118 # pb for 1200 GeV Tprime  nevts_blah = 253993</p> <p>obsbins=np.arange(500,2050,80)</p> <p>lumi uncertainty in 2015 - 2.2% </p>"},{"location":"setup/","title":"Setup","text":"<p>To complete these exercises, we will be using two container images, with the software installed for you. In the examples here, we will use Docker to run the images. The Docker desktop is available for mac, windows and linux so follow the link and download the right installation for your personal laptop. You should start by downloading the Docker desktop for your laptop (click here and follow the instructions). You will need to setup an account to do so. </p> <p>Once you have the Docker desktop installed, make sure it is running and download the two containers that we'll need for the exercises using the terminal commands below. Note that the Docker desktop has its own terminal if you prefer to use that. </p>"},{"location":"setup/#python-environment-for-cms-open-data-datasets","title":"Python environment for CMS Open Data datasets","text":"<p>Obtain the <code>cms_python</code> container using, </p> Bash<pre><code>docker run -it --name cms_python -P -p 8888:8888 -v ${HOME}/cms_open_data_python:/code gitlab-registry.cern.ch/cms-cloud/python-vnc:python3.10.5\n</code></pre> <p>Now that you're inside the container, run the following to get all of the necessary scripts and install some additional packages.  Bash<pre><code>pip install vector coffea==0.7.21\n</code></pre></p> <p>You can exit the container at any time by typing <code>exit</code> in the terminal. To restart the python container, open a terminal and enter  Bash<pre><code>docker start -i cms_python\n</code></pre></p>"},{"location":"setup/#combine-package-for-statistical-analysis","title":"Combine package for statistical analysis","text":"<p>Obtain the <code>cms_combine</code> container using, </p> Bash<pre><code>docker run -p 127.0.0.1:8889:8889 --name cms_combine -it gitlab-registry.cern.ch/cms-cloud/combine-standalone:v9.2.1-slim\n</code></pre> <p>If you like to make plots with python instead of using ROOT, then you should also install <code>matplotlib</code> inside this container by running the following command in the terminal inside the container.  Bash<pre><code>pip install matplotlib \n</code></pre></p> <p>You can exit the container at any time by typing <code>exit</code> in the terminal.  To restart the combine container, open a terminal and enter  Bash<pre><code>docker start -i cms_combine\n</code></pre></p>"},{"location":"setup/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>As much as possible, we will use Juptyer notebooks to write and run code for these exercises. You can launch JupterLab from either of the two containers by using the following command inside the terminal after starting the container. </p> Bash<pre><code>jupyter lab --ip 0.0.0.0 --port XXXX --no-browser\n</code></pre> <p>where <code>XXXX</code> should be <code>8888</code> for the python container, and <code>8889</code> for the combine container. </p> <p>The output will give you a link that you can paste in your preferred internet browser to open the JupyterLab environment. The output will look similar to the one below,  </p> <p>Copy one of the URL paths into the URL bar in your preferred browser. You should see something like the following. In this example I have  launched JupyterLab from inside the combine container, </p> <p></p> <p>From here, we can open a new terminal, text file or notebook. On the left you can see a file browser that shows all of the files contained in the container. You can modify/copy/delete these using the file browser by right clicking on them.</p>"},{"location":"setup/#notebooks","title":"Notebooks","text":"<p>Selecting a \"Python 3\" Juypter notebook allows you to run code python and see the output all in one page. You can type code into a cell and run it by either pressing the play button ( \u25ba) at the top or using <code>shift</code>+<code>enter</code> on your keyboard, as in the image below </p> <p></p> <p>Create new cells by using the <code>+</code> button at the top. If you press the fast-fwd \u23e9  button at the top, all of the cells in the whole notebook will be run one after the other. Notebooks will automatically save every so often so your work will be kept. The files will be named with the <code>.ipynb</code> extension.</p>"},{"location":"setup/#terminal","title":"Terminal","text":"<p>You will need to use the terminal for some commands. For this, you can either use the same terminal as you used to start docker, or you can open a new \"Terminal\" in your browser from the Launcher tab, </p> <p></p> <p>Warning</p> <p>The browser based terminal inside JupyterLab is very useful but is defaulted to use a fairly cumbersome shell. If you use this terminal, you might find it helpful to first type <code>bash</code> into the terminal to use the BASH terminal shell, which is much better. </p>"},{"location":"setup/#csv-files","title":"CSV files","text":"<p>Since our data structures will be mostly Pandas dataframes, you might want to install the Jupyter spreadsheet editor that allows you to both view and edit CSV files,  Bash<pre><code>pip install jupyterlab-spreadsheet-editor\n</code></pre> You can do this in both of the containers but you don't need to install this for the exercises.</p>"},{"location":"setup/#moving-files-around","title":"Moving files around","text":"<p>From time to time, we will need to move files between containers or to our own computers. You can do this by downloading the file from the browser (right click on a file in the file browser and select \"download\") or you can use the <code>docker cp</code> tool. For example to transfer a file called \"myfile.txt\" from your local desktop to the <code>cms_python</code> container, you can run, </p> Bash<pre><code>docker cp ~/Desktop/myfile.txt cms_python:/code/\n</code></pre> <p>in a terminal on your local machine. You should see a message with the words <code>Sucessfully copied ...</code> if everything worked.  You can also copy files from the containers to your local machine by reversing the order of the locations in the command above, and you can copy files between containers. </p> <p>Warning</p> <p>With the way we are downloading and starting the containers, anything you create will still be there the next time you start the container. However, if you delete the container from Docker desktop then everything you did will be gone! You should make sure you copy any files/work you want to keep to your local computer using the <code>docker cp</code> commands above before you permenantly delete the containers. </p>"}]}