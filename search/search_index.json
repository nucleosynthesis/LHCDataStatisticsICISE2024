{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"INTRODUCTION","text":"<p>This is the webpage for the lecture series on Data Analysis and Statistics at the LHC. In these pages, you can find links to the lectures as well as exercises for you to explore a typical data analysis at CMS.  You should be familiar with simple programming in Python, however, the examples should give you enough information to produce the desired output even if you don't know much Python. </p> <p>You should start by downloading the Docker desktop for your laptop (click here). You will need to setup an account to do so. Follow the instructions for setting up the software that we'll be using at the Getting started tab. This can take some time so please do this before the start of the lecture course. </p> <p>There are four sets of exercises in these pages that you can find under the Exercises tab, one for each day of the course. You should read through the information given and copy code/commands where prompted into your own terminal or notebook. </p> <p>Question</p> <p>Throughout the exercises, there will be questions in a green box like this that contains a challenge for you to try on your own. Below, there will be a solution in a blue box for you to check against your own answer or in case you get really stuck. Please do try on your own before revealing the solution. </p> Show answer The answer will be shown here  <p>Challenge</p> <p>If you are feeling especially confident, you can have a go at extra challenges that I have put in these turquoise boxes. I won't provide a solution for these challenges, but you don't need to complete them to progress through the other exercises. </p> <p>Warning</p> <p>Sometimes, there will be some technical or conceptual hurdle that requires a bit of careful thought to work around. I have tried to point these out to you in orange boxes like this one to help avoid any major road blocks. </p>"},{"location":"#useful-links","title":"Useful Links","text":"<p>You can find more useful information about the software we will be using below </p> <ul> <li>Docker: Docker is a desktop client for running the container images that we will be using for these exercises.  </li> <li>CMS Open Data: The CMS Collaboration regularly releases open datasets for public use. This link is the CMS Open Data Guide</li> <li>Combine: The Combine tool is the main statistical analysis software package used by the CMS Collaboration. It is based on the ROOT/RooFit software packages, however for these exercises, you do not need to be an expert user of these packages. </li> </ul>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Dr. Nicholas Wardle is a lecturer in Physics at Imperial College London. He is also a member of the CMS Collaboration and his research is focused on statistical methods for data analysis and searches for BSM physics through precision measurements of Higgs boson properties. </p>"},{"location":"day1/","title":"Exercise 1 - Datasets, Selections and Histograms","text":"<p>For the exercises, we will use a simplified version of a CMS analysis to measure the production cross-section of top-quark / anti-top-quark pairs - \\sigma_{tt} - in proton-proton collisions at a centre of mass energy of 13 TeV. We will be using real data from the CMS experiment, taken in 2015 during Run 2 of the LHC. The code is based on the CMS open data workshop linked here, which you can read through if you are interested. </p> <p>The image below is a Feynman diagram of the process we are interested in measuring, namely t\\bar{t}\\rightarrow (bW^{+})(\\bar{b}W^{-})\\rightarrow bl^{+}\\nu_{l}\\,\\bar{b}q\\bar{q} - the lepton+jets final state. </p> <p></p> <p>Of course, the charge conjugate version (with the sign of the charged lepton flipped) is also possible. In the following, we will consider both versions so don't get too hung up on the charges of the particles in the final state. We'll drop the charge and anti-particle symbols from here-on. </p> <p>In our example, we will focus on the case where the lepton l is an electron or muon.  In the CMS detector, these leptons are extremely well measured and are very good for \"triggering\" the events that we are interested in. Looking at the final state, we would also expect to see 4 hadronic jets, two of which will be b-jets, and missing transverse momentum from the neutrino which isn't detected.  </p>"},{"location":"day1/#launch-jupyterlab","title":"Launch JupyterLab","text":"<p>For this exercise, we will be using data from the LHC Run-2 proton-proton collision data recorded by CMS during 2015.  In addition to the data, we will be using simulated samples of tt\\rightarrow (bW)(bW)\\rightarrow bl\\nu_{l}\\,bqq production and various background processes that can mimic the signal.  </p> <p>First, start the <code>cms_python</code> container using  <pre><code>docker start -i cms_python\n</code></pre> Note that you should have the Docker desktop running on your laptop for this to work. If you didn't install the container already, please see the Getting started pages before continuing. </p> <p>Next, you should checkout the GitHub area that contains some helpful files and information for accessing and processing the data. In the same terminal as you started the container in type,  <pre><code>git clone https://github.com/nucleosynthesis/LHCDataStatisticsICISE2024.git\ncd LHCDataStatisticsICISE2024/ttbarAnalysis \n</code></pre></p> <p>Now you can start the JupyterLab server by running  <pre><code>jupyter lab --ip 0.0.0.0 --port 8888 --no-browser\n</code></pre></p> <p>The output will give you a link (starting with <code>http://</code>) that you can paste into your preferred internet browser to open the JuypterLab session. Create a new Python 3 notebook - icon that says Python 3 (ipykernel) - and give it a name by right clicking in the file browser at the left panel where it says \"Untitled.ipynb\". We will use this notebook to issue commands to process the data. </p> <p>You can create new cells in the notebook by using the \"+\" icon at the top. The code that is entered in each cell can be run by either clicking the play icon at the top or by selecting the cell and pressing Shift + Enter on your keyboard. Jupyter notebooks are a great way of editing code and displaying output all in one place! </p>"},{"location":"day1/#accessing-and-exploring-the-data-sets","title":"Accessing and exploring the data sets","text":"<p>In your JupyterLab file browser, you will see the list of files that were obtained from GitHub. There is a file called <code>ntuples.json</code> that contains the file paths for the different samples (data and simulation) that we'll use in these exercises. </p> <p>In the file browser, if you click on the <code>ntuple.json</code> file, it will open in the main panel. You can click on the different names to expand them and see the information contained in the file. It should look like the image below, </p> <p></p> <p>These datasets have been skimmed by applying certain selection criteria. This is to make the samples more manageable as the original datasets are TBs in size! The requirements applied are, </p> <ul> <li>That the events fired at least one of these triggers: <code>HLT_Ele22_eta2p1_WPLoose_Gsf</code>, <code>HLT_IsoMu20_v</code>, <code>HLT_IsoTkMu20_v</code>. These essentially require that the online selection (trigger) found at least one isolated electron or one isolated muon with a minimum p_{T}, in a certain part of the CMS detector (can you figure out from the names, what the p_{T} requirements at the online selection are?). </li> <li>That the event contains either at least one tight electron or at least one tight muon with p_{T}&gt; 26 GeV and |\\eta|&lt;2.1. When we use the word tight in this context, it is jargon internal to CMS that just specifies how pure we are trying to make the sample. A tight selection criteria would be quite pure and have fewer false positives than a medium or loose selection which might retain more true muons or electrons, but also have more false positives (fake electrons or muons). </li> </ul> <p>The data are saved as <code>.root</code> files. Don't worry if you are not familiar with the ROOT software, we will some very nice Python tools from the Hep Software Foundation to read these files and convert them into Python based formats. </p> <p>Copy the code below into a cell in your Jupyter notebook. This imports the various modules that we need to read and convert our data into Python formats. </p> <pre><code>import uproot\nimport numpy as np\nimport awkward as ak\nfrom coffea.nanoevents import NanoEventsFactory, BaseSchema\nfrom agc_schema import AGCSchema\n</code></pre> <p>Run the cell to execute the commands. You should see the cell number (eg <code>[1]</code>) become an asterisk (<code>[*]</code>) for a short while, which indicates that the cell is running. Once the commands have finished, the number will appear again. </p> <p>Next, let's open one of the files from the <code>ttbar</code> sample. Copy the command below into the next cell of your Jupyter notebook and run it. </p> <pre><code>events = NanoEventsFactory.from_root('root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/RunIIFall15MiniAODv2_TT_TuneCUETP8M1_13TeV-powheg-pythia8_flat/00EFE6B3-01FE-4FBF-ADCB-2921D5903E44_flat.root', schemaclass=AGCSchema, treepath='events').events()\n\nprint(events.fields)\n</code></pre> <p>The output should be a list of the different collections (fields) that are contained in the data. Each event may have a different number of objects in each collection. We can see this by entering the following code in the Jupyter notebook cell, </p> <pre><code>print(\"Electrons:\")\nprint(f\"Number of events: {ak.num(events.electron, axis=0)}\")\nprint(ak.num(events.electron, axis=1))\nprint()\n</code></pre> <p>The output should look like  <pre><code>Electrons:\nNumber of events: 325519\n[3, 2, 2, 1, 1, 2, 0, 1, 2, 3, 1, 1, 3, 4, ... 4, 1, 1, 2, 5, 3, 1, 2, 2, 2, 1, 2, 1]\n</code></pre></p> <p>From this we can see that there are 325519 events in total in this file and each event has a different number of electrons. Notice that we used the same <code>ak.num</code> command in each printout but we changed the depth of the sum by specifying <code>axis=</code>. The larger this number, the deeper into the event structure the function goes. </p> <p>Question</p> <p>Do the same for <code>muon</code>, <code>jet</code> and <code>met</code> in each event, to see how many of these objects are contained in each event. Why do you get an error when using the function for <code>met</code>, the missing transverse momentum of the event?  </p> Show answer  Similarly to the case for electrons, we can use the following,  <pre><code>print(\"Muons:\")\nprint(f\"Number of events: {ak.num(events.muon, axis=0)}\")\nprint(ak.num(events.muon, axis=1))\nprint()\n\nprint(\"Jets:\")\nprint(f\"Number of events: {ak.num(events.jet, axis=0)}\")\nprint(ak.num(events.jet, axis=1))\n</code></pre>  Which yields the output,  <pre><code>Muons:\nNumber of events: 325519\n[1, 1, 1, 0, 6, 6, 3, 2, 1, 2, 2, 6, 4, 2, ... 2, 1, 1, 2, 0, 5, 2, 3, 2, 2, 2, 2, 1]\n\nJets:\nNumber of events: 325519\n[4, 5, 4, 5, 9, 4, 4, 8, 5, 9, 5, 4, 6, 6, ... 6, 2, 4, 3, 5, 9, 4, 7, 4, 4, 6, 5, 4]\n</code></pre>  When doing the same for <code>met</code>, we get an error  <pre><code>ValueError: 'axis' out of range for 'num'\n</code></pre>  The reason is that the <code>met</code> field is an event level variable meaning it is calculated only once per event."},{"location":"day1/#applying-a-basic-analysis-selection","title":"Applying a basic analysis selection","text":"<p>We will apply selections to the events to discard those that are unlikely to have arisen from our target (signal) process t\\bar{t}\\rightarrow (bW^{+})(bW^{-})\\rightarrow bq\\bar{q}bl^{-}\\bar{\\nu}_{l}. </p> <p>Looking at the process, its clear that we should select events that have one muon or one electron, along with at least four jets which are either from a light flavour quark or from a b-quark (q or b). </p> <p>Generally, when applying selections to events, we first think about object selection - i.e we only want to apply event selections using well identified objects, and then event selection where we really target the final state that we are looking for. </p>"},{"location":"day1/#object-selection","title":"Object selection","text":"<p>We can see which features each of our collections have by checking the <code>fields</code> available. Copy the following code into a Juptyer notebook cell,  <pre><code>print(\"Electron fields:\")\nprint(events.electron.fields)\nprint()\n</code></pre> This should give the following output, </p> <pre><code>Electron fields:\n['pt', 'px', 'py', 'pz', 'eta', 'phi', 'ch', 'iso', 'veto', 'isLoose', 'isMedium', 'isTight', 'dxy', 'dz', 'dxyError', 'dzError', 'ismvaLoose', 'ismvaTight', 'ip3d', 'sip3d', 'energy']\n</code></pre> <p>We can see that the electrons have fields corresponding to, </p> <ul> <li>The kinematics of the electron: transverse momentum <code>pt</code> (p_{T}), pseudo-rapidity <code>eta</code> (\\eta)</li> <li>Identification quality information : Whether the electron passes loose, medium or tight selection requirements : <code>isLoose</code>, <code>isMedium</code>, <code>isTight</code>. </li> <li>Other features related to the track-calorimeter matching and how well isolated the electron object is. </li> </ul> <p>We can apply requirements on the objects by applying masks to our event collections. For electrons, we will apply the following criteria to the electron fields, </p> <ul> <li>p_{T}&gt; 30 GeV, |\\eta|&lt;2.1</li> <li>passing the tight identification requirements </li> <li>3D impact parameter (<code>sip3d</code>) &lt; 4</li> </ul> <p>Copy the code below to determine which events would pass this selection,  <pre><code>selected_electrons = events.electron[(events.electron.pt &gt; 30) &amp; (abs(events.electron.eta)&lt;2.1) &amp; (events.electron.isTight == True) &amp; (events.electron.sip3d &lt; 4 )]\n</code></pre></p> <p>Question</p> <p>Create the same object masks for selected muons and selected jets based on the following criteria, </p> <p>For muons: </p> <ul> <li>p_{T}&gt; 30 GeV, |\\eta|&lt;2.1 </li> <li>passing the tight identification requirements  </li> <li>3D impact parameter (<code>sip3d</code>) &lt; 4</li> <li>Relative isolation requirement (<code>pfreliso04DBCorr</code>) &lt; 0.15.</li> </ul> <p>For jets: </p> <ul> <li>Corrected (<code>corrpt</code>) p_{T}&gt; 30 GeV and |\\eta|&lt;2.4 </li> </ul> Show answer <pre><code>selected_muons = events.muon[(events.muon.pt &gt; 30) &amp; (abs(events.muon.eta)&lt;2.1) &amp; (events.muon.isTight == True) &amp; (events.muon.sip3d &lt; 4) &amp; (events.muon.pfreliso04DBCorr &lt; 0.15)]\nselected_jets = events.jet[(events.jet.corrpt &gt; 30) &amp; (abs(events.jet.eta)&lt;2.4) ]\n</code></pre>"},{"location":"day1/#event-selection","title":"Event selection","text":"<p>Now we want to apply the event level filters to our events. To do this, we use the <code>ak.count</code> method to count the number of objects of each type that we require in our events. Looking at our final state, our event selection will be, </p> <ul> <li>Exactly one electron or muon that passes the electron/muon requirements </li> <li>At least 4 jets, at least two of which should be a b-jet. </li> </ul> <p>For the first requirement, copy the following code into a Jupyter notebook cell and run it,  <pre><code>event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n</code></pre> The <code>+</code> acts as a logical or so that we are filtering on events that have exactly one electron or exactly one muon. Note that we could use another field of the object but here we have chosen to use <code>pt</code>. Remember, we want to apply the filter on each electron or muon in the events so the sum needs to be applied to the first layer (<code>axis=1</code>). </p> <p>For the second requirement, we need to define how we tag a b-jet. We use the combined secondary vertex algorithm at CMS to do this. When this quantity is high, this implies that the jet originates from a vertex which is far from the pp interaction point, which indicates that the jet originates from a heavy quark with a long(ish) lifetime - i.e a b-quark. For our selection  we will define b-jets as those for which the  b-tagging variable <code>btag</code> is greater than or equal to 0.8. </p> <p>The following code, which you should copy into a cell in your Jupyter notebook, will include the second requirement into our event filters, </p> <pre><code>event_filters = event_filters &amp;  (ak.count(selected_jets.corrpt, axis=1) &gt;= 4) &amp; (ak.sum(selected_jets.btag &gt;= 0.8, axis=1) &gt;= 2)\n</code></pre> <p>Note that we haven't actually applied any of this selection to our events yet. If we check the length of our event filters, we can see that its still the same size as our events,  <pre><code>print(len(event_filters))\n</code></pre></p> <p>The reason is that this object now contains a boolean flag, one for each event, that indicates whether or not the event satisfies all of the criteria that we used to define the filter. To actually select the events that pass this filter, we just apply the filter to one of our selected object collections. For example, to obtain the collection of jets from all of the events that pass our selection, we can do, </p> <pre><code>selected_events = selected_jets[events_filter]\n</code></pre> <p>As a final step, we want to plot some observable in our events that can separate the signal from the various backgrounds. Choosing observables often requires a lot of thought as physicists as there are several things we want to consider </p> <ul> <li>The observable should have some separation between signal and background to make it easier to extract the process that we are interested in. </li> <li>The observable may be more or less susceptible to experimental effects such as detector resolution, pile-up or be theoretically more prone to divergences in the calculation (this can especially true in jet related observables). All of these tend to imply systematic uncertainties that can reduce the sensitivity. </li> <li>The observable may lend itself to allowing for data-driven estimates of distributions for different backgrounds. This won't be the case here, but this can often be a good motivation for choosing certain variables. </li> </ul> <p>For this analysis, our chosen observable will be the mass of the hadronically decaying top-quark m_{bjj}.  We can use the <code>ak.combinations</code> function to find all combinations of 3-jets (trijet) in the events. For each combination, we will calculate the four-momentum of the 3-jet system and require that at least one of the 3 jets in each trijet is b-tagged. </p> <p>Copy the code below into a Jupyter notebook cell and run it, </p> <pre><code>trijet = ak.combinations(selected_events, 3, fields=[\"j1\", \"j2\", \"j3\"])\n\n# Get the individual jets from each combination \nj1,j2,j3 = ak.unzip(trijet)\n# booleans for which combination has one of the 3 jets b-tagged\nhas_bjet = (ak.sum(j1.btag&gt;=0.8,axis=1)+ak.sum(j2.btag&gt;=0.8,axis=1)+ak.sum(j3.btag&gt;=0.8,axis=1)&gt;0)\n# apply the filter \ntrijet = trijet[has_bjet]\n\ntrijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3 \n</code></pre> <p>Note that in the last command, <code>p4</code> is a special field that has the space-time co-ordinates of a four-vector. This makes our calculations of things like the p_{T} or mass of the trijets very easy. Below, we find the invariant mass of the trijet that has the highest p_{T}. </p> <pre><code>trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n</code></pre> <p>Question</p> <p>Create an observable for the p_{T} of the trijet with the largest p_{T}. </p> Show answer <pre><code>trijet_pt = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].pt\n</code></pre>"},{"location":"day1/#histograms-and-saving-output","title":"Histograms and saving output","text":"<p>Now that we have our event observable, we can see what the distribution looks like for these events.  First, we need to remove the array structure of the events so that we can use the plotting tools from matplotlib. We have to import this module too of course,  <pre><code>import matplotlib.pyplot as plt\nobservable = ak.flatten(trijet_mass).to_numpy()\nplt.hist(observable,bins=np.arange(50,575,25))\n</code></pre></p> <p>You should see a plot similar to the one below,  </p> <p>We see a peak close to the top mass (~175 GeV) as expected! The resolution of jets in the CMS detector is not very high so we do not see a very sharp peak, but we are clearly seeing the hadronic top quark here. </p> <p>Question</p> <p>Make a plot of the p_{T} of the trijet instead of the mass. You should be careful to choose appropriate binning for this variable. </p> Show answer <pre><code>plt.hist(observablept,bins=np.arange(0,300,25))\n</code></pre> <p>Challenge</p> <p>In the original CMS paper, the observable used was the invariant mass of the b-jet and the lepton (electron or muon) - m_{bl}. For the case where there is more than 1 b-jet in particular the minimum such mass from all pairings of b-jet with the lepton is used. Create this observable for the selected events and plot a histogram of it for our ttbar sample. </p> <p>As this sample is simulation, we need to apply proper event weights to account for the cross-section in the standard model. Remember that the number of events expected is given by the product of the cross-section and the integrated luminosity. For simulated samples, if we know the cross-section \\sigma and the number of events generated N_{gen}, we can calculate the effective luminosity of the simulated sample as, </p>  L_{eff}  =  N_{gen}/\\sigma  <p>Often, there is a filtering step in either the MC generation or in the skimming step - we don't always manage to process all of the samples. It's important to account for this by scaling up the number of generated events by this filter efficiency \\epsilon. So our formula changes to </p>  L_{eff}  =  \\frac{1}{\\epsilon}\\cdot N_{gen}/\\sigma  <p>The event weight for a particular sample will therefore be the ratio of the integrated luminosity of our real data to the effective luminosity of the sample, </p>  w = L_{int}/L_{eff} =L_{int}\\cdot \\epsilon\\cdot \\sigma/N_{gen}   <p>Have a look again inside the <code>ntuples.json</code> file. You will find that the file contains the number of events for each file in each sample - <code>nevts</code> -  (this is the number generated and processed to make that file), the filter efficiency for the sample (<code>filter_eff</code>) and the cross-section in pb - <code>xsec</code> for that sample. You can read in this data using the code below, </p> <p><pre><code>import json\nwith open('ntuples.json', 'r') as f:\n    metadata = json.load(f)\n</code></pre> From this information, you can now calculate the event weight for each of the simulated samples. Our integrated luminosity for this 2015 data is 2256.38 pb. </p> <p>Question</p> <p>Write a function to calculate the event weight (<code>getEventWeight</code>) for a given simulated sample, using the metadata from <code>ntuples.json</code>.  </p> Show answer <pre><code>def getEventWeight(sample,file_index):\n    lumi = 2256.38 # inv pb \n    if sample != \"data\":\n        xsec_weight = metadata[sample]['filter_eff'] * metadata[sample]['xsec'] * lumi / metadata[sample]['nominal'][file_index]['nevts']\n    else:\n        xsec_weight = 1\n    return xsec_weight\n</code></pre> <p>Finally, we want to save our histogram in a format that can be interpreted by the <code>Combine</code> tool that we use for statistical analyses in CMS. There are many different formats that can be used, however we will use a simple <code>.csv</code> format for these exercises. </p> <p>I have created a simple conversion function for you to use in the file <code>hist2df.py</code>. It's also copied below, </p> Show python <pre><code>import pandas as pd\n\ndef histogramToDataframe(weights,channel,process,sys='nominal'):\n\n    df = {'channel':[],'process':[],'systematic':[],'bin':[],'sum_w':[],'sum_ww':[]}\n    nbins = len(weights)\n    df['bin']=range(nbins)\n    df['channel']=[channel for i in range(nbins)]\n    df['process']=[process for i in range(nbins)]\n    df['systematic']=[sys for i in range(nbins)]\n    df['sum_w']=list(weights)\n    df['sum_ww']=list(weights)\n    ret = pd.DataFrame.from_dict(df)\n    return ret\n</code></pre> <p>You can convert a histogram created with the matplotlib <code>hist</code> function to a dataframe using this function as follows, </p> <pre><code>weights =  plt.hist(observable,bins=np.arange(50,575,25))\ndfs = histogramToDataframe(weights[0],\"signalregion\",\"ttbar\")\n</code></pre> <p>Since we haven't included the event weights when making the histogram, we will multiply the column <code>sum_w</code> by this weight.  <pre><code>event_weight  = getEventWeight('ttbar',0) # the weight needed when using only the first file \ndfs['sum_w']  *= event_weight \ndfs['sum_ww'] *= event_weight*event_weight\n</code></pre></p> <p>Note that there is also a column in our dataframe called <code>sum_ww</code> that we have multiplied by the square of the event weight. This column is the variance of the bin contents due to the limited number of MC simulated events available. We'll discuss more about this column in later exercises. </p> <p>Finally, we can save the histogram in the format needed later for <code>Combine</code> using,  <pre><code>dfs.to_csv('histograms.csv',index=False)\n</code></pre> And you'll now see a file called <code>histograms.csv</code> in your file browser. </p>"},{"location":"day1/#full-analysis","title":"Full analysis","text":"<p>In this exercise, we only ran our selection and created a histogram for one of the files for one of our simulated processes. We should run the same selection over all of the simulated files and for the data too!</p> <p>Question</p> <p>Write some code that will run over all of the simulated sample files and all of the data files too. Remember, </p> <ol> <li>Only look at the <code>nominal</code> samples. </li> <li>Make a separate histogram for each of the different processes and for the data. Each simulated sample will have its own event weight that you'll need to calculate. </li> <li>You can save all of the histograms to the same <code>.csv</code> file or you can save them in separate files.</li> </ol> <p>This may take some time so you should get some working code and then you may need to let the commands run for a couple of hours - maybe leave it running over dinner or even overnight. </p> <p>Feel free to choose your own binning for your observable or you could even choose a different observable to the one we calculated in the example above! You might even try saving different histograms with different binnings/observables so that you can see what difference this makes for the statistical analysis later. </p> <p>Please try to write your own code before looking at the answer provided. </p> <p>Warning</p> <p>You may find that sometimes you will see an <code>IOError</code> due to XRootD failing to access the remote files. This can be annoying so I recommend that you split up the task across different cells and that you wrap the command to open the file with a <code>try</code> <code>except</code> block similar to,  <pre><code>input_file = \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/POET/23-Jul-22/Run2015D_SingleMuon_flat/07FC2CD2-106C-4419-9260-12B3E271C345_flat.root\"\ntry: \n    events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\nexcept OSError:\n    time.sleep(2) # sleep for 2 seconds and try one more time\n    try: \n        events = NanoEventsFactory.from_root(input_file, schemaclass=AGCSchema, treepath='events').events()\n    except OSError:\n        time.sleep(2) # sleep for 2 seconds just to not try EOS again but give up on this file now\n        return []\n</code></pre> This will try to access a file twice before giving up and returning an empty list.</p> <p>Remember, if you don't access the full set of files, you should recalculate the integrated luminosity (you can assume that the new integrated luminosity scales with the number of events accessed) and modify the function to calculate the event weight to account for missing simulated events. </p> Show answer <p>I have uploaded a Jupyter notebook that will perform the full analysis on the samples called <code>FullAnalysis.ipynb</code>in the <code>ttbarAnalysis</code> folder. If you really get stuck, have a look at this notebook to see how to run our object and event selection, calculate the observable and save the histograms for all of the samples. </p> <p>The notebook will produce a plot similar to the one below,</p> <p></p>"},{"location":"day2/","title":"Exercise 1 - Maximum Likelihood Fits","text":"<p>From yesterday's exercise, we now have a set of histograms, from data and simulation, in our <code>.csv</code> file. Don't worry if you didn't manage to run over all of the samples from yesterday, you can use the pre-prepared file <code>ttbarAnalysis/exercise1solutions/signalregion_mbjj.csv</code>.</p> <p>In today's exercise </p> CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"setup/","title":"Setup","text":"<p>To complete these exercises, we will be using two container images, with the software installed for you. In the examples here, we will use docker to run the images. The docker desktop is available for mac, windows and linux so follow the link and download the right installation for your personal laptop. </p> <p>Once you have the docker desktop installed, make sure it is running and get the two containers that we'll need for the exercises using the terminal commands below. Note that the docker desktop has its own terminal if you prefer to use that one. </p>"},{"location":"setup/#python-environment-for-cms-open-data-datasets","title":"Python environment for CMS Open Data datasets","text":"<p>Obtain the <code>cms_python</code> container using, </p> <pre><code>docker run -it --name cms_python -P -p 8888:8888 -v ${HOME}/cms_open_data_python:/code gitlab-registry.cern.ch/cms-cloud/python-vnc:python3.10.5\n</code></pre> <p>Now that you're inside the container, run the following to get all of the necessary scripts and install some additional packages.  <pre><code>pip install vector coffea==0.7.21\n</code></pre></p> <p>You can exit the container at any time by typing <code>exit</code> in the terminal. To restart the python container, open a terminal and enter  <pre><code>docker start -i cms_python\n</code></pre></p>"},{"location":"setup/#combine-package-for-statistical-analysis","title":"Combine package for statistical analysis","text":"<p>Obtain the <code>cms_combine</code> container using, </p> <pre><code>docker run -p 127.0.0.1:8889:8889 --name cms_combine -it gitlab-registry.cern.ch/cms-cloud/combine-standalone:v9.2.1-slim\n</code></pre> <p>If you like to make plots with python instead of using ROOT, then you should also install <code>matplotlib</code> inside this container by running the following command in the terminal inside the container.  <pre><code>pip install matplotlib \n</code></pre></p> <p>You can exit the container at any time by typing <code>exit</code> in the terminal.  To restart the combine container, open a terminal and enter  <pre><code>docker start -i cms_combine\n</code></pre></p>"},{"location":"setup/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>As much as possible, we will use Juptyer notebooks to write and run code for these exercises. You can launch JupterLab from either of the two containers by using the following command inside the terminal after starting the container. </p> <pre><code>jupyter lab --ip 0.0.0.0 --port XXXX --no-browser\n</code></pre> <p>where <code>XXXX</code> should be <code>8888</code> for the python container, and <code>8889</code> for the combine container. </p> <p>The output will give you a link that you can paste in your preferred internet browser to open the JupyterLab environment. You should see something like the following for example when launching JupyterLab from the combine container, </p> <p></p> <p>From here, we can open a new terminal, text file or notebook. On the left you can see a file browser that shows all of the files contained in the container. You can modify/copy/delete these using the file browser by right clicking on them.</p> <p>Since our data structures will be mostly Pandas dataframes, you might want to install the Jupyter spreadsheet editor that allows you to both view and edit CSV files,  <pre><code>pip install jupyterlab-spreadsheet-editor\n</code></pre> You can do this in both of the containers but you don't need to install this for the exercises. </p>"},{"location":"setup/#moving-files-around","title":"Moving files around","text":"<p>From time to time, we will need to move files between containers or to our own computers. You can do this by downloading the file from the browser (right click on a file in the file browser and select \"downlowd\") or you can use the <code>docker cp</code> tool. For example to transfer a file called \"myfile.txt\" from your local desktop to the <code>cms_python</code> container, you can run, </p> <pre><code>docker cp ~/Desktop/myfile.txt cms_python:/code/\n</code></pre> <p>in a terminal on your local machine. You can also copy files from the containers to your local machine by reversing the order of the locations in the command above. </p>"}]}